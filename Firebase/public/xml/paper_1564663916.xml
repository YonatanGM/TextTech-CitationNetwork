<paper id="1564663916"><title>Learning to Take Actions</title><year>1999</year><authors><author org="Division of Informatics, University of Edinburgh, JCMB, Kingâ€˜s Buildings, Edinburgh EH9 3JZ, Scotland. roni@dcs.ed.ac.uk#TAB#" id="639666141">Roni Khardon</author></authors><n_citation>74</n_citation><doc_type>Journal</doc_type><references><reference>75224112</reference><reference>123943295</reference><reference>139557157</reference><reference>159210726</reference><reference>1491843047</reference><reference>1502259354</reference><reference>1520252399</reference><reference>1528869148</reference><reference>1537458137</reference><reference>1539216098</reference><reference>1555519929</reference><reference>1555696599</reference><reference>1600911950</reference><reference>1608763299</reference><reference>1612069379</reference><reference>1619748997</reference><reference>1680127546</reference><reference>1699699942</reference><reference>1766442844</reference><reference>1966028617</reference><reference>1967346767</reference><reference>1987902506</reference><reference>1989445634</reference><reference>1999138184</reference><reference>2004683093</reference><reference>2008590749</reference><reference>2011039300</reference><reference>2019363670</reference><reference>2035078485</reference><reference>2036265926</reference><reference>2061146398</reference><reference>2062686286</reference><reference>2078226168</reference><reference>2083143894</reference><reference>2083208676</reference><reference>2095995891</reference><reference>2098103640</reference><reference>2100677568</reference><reference>2101602574</reference><reference>2103537992</reference><reference>2107726111</reference><reference>2109090232</reference><reference>2110415190</reference><reference>2111438441</reference><reference>2115005659</reference><reference>2117049614</reference><reference>2117555988</reference><reference>2122308921</reference><reference>2129113961</reference><reference>2129192653</reference><reference>2131600418</reference><reference>2133422947</reference><reference>2133871179</reference><reference>2134980541</reference><reference>2141288588</reference><reference>2145323224</reference><reference>2149611281</reference><reference>2154297071</reference><reference>2158742492</reference><reference>2161397207</reference><reference>2164457256</reference><reference>2166559705</reference><reference>2166770232</reference><reference>2172074164</reference><reference>2180885055</reference><reference>2428981601</reference><reference>2912171497</reference></references><venue id="62148650" type="J">Machine Learning</venue><doi>10.1023/A:1007571119753</doi><keywords><keyword weight="0.57842">Online machine learning</keyword><keyword weight="0.57605">Algorithmic learning theory</keyword><keyword weight="0.59182">Semi-supervised learning</keyword><keyword weight="0.57997">Instance-based learning</keyword><keyword weight="0.5779">Stability (learning theory)</keyword><keyword weight="0.56539">Unsupervised learning</keyword><keyword weight="0.46649">Artificial intelligence</keyword><keyword weight="0.5746">Computational learning theory</keyword><keyword weight="0.46147">Machine learning</keyword><keyword weight="0.4057">Mathematics</keyword><keyword weight="0.57432">Learning classifier system</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>We formalize a model for supervised learning of action strategies in dynamic stochastic domains and show that PAC-learning results on Occam algorithms hold in this model as well. We then identify a class of rule-based action strategies for which polynomial time learning is possible. The representation of strategies is a generalization of decision listss strategies include rules with existentially quantified conditions, simple recursive predicates, and small internal state, but are syntactically restricted. We also study the learnability of hierarchically composed strategies where a subroutine already acquired can be used as a basic action in a higher level strategy. We prove some positive results in this setting, but also show that in some cases the hierarchical learning problem is computationally hard.</abstract></paper>