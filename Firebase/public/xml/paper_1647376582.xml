<paper id="1647376582"><title>On the influence of the kernel on the consistency of support vector machines</title><year>2002</year><authors><author org="Mathematisches Institut, Friedrich-Schiller-UniversitÃ¤t, Ernst-Abbe-Platz 1-4, 07743 Jena, Germany#TAB#" id="37742912">Ingo Steinwart</author></authors><n_citation>411</n_citation><doc_type>Journal</doc_type><references><reference>1485026097</reference><reference>1540155273</reference><reference>2106491486</reference><reference>2168420538</reference></references><venue id="118988714" type="J">Journal of Machine Learning Research</venue><doi>10.1162/153244302760185252</doi><keywords><keyword weight="0.51225">Kernel (linear algebra)</keyword><keyword weight="0.4937">Polynomial</keyword><keyword weight="0.45576">Pattern recognition</keyword><keyword weight="0.55994">Support vector machine</keyword><keyword weight="0.49375">Regularization (mathematics)</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.49786">Computational learning theory</keyword><keyword weight="0.59481">Margin classifier</keyword><keyword weight="0.57053">Kernel method</keyword><keyword weight="0.50468">Optimization problem</keyword><keyword weight="0.44473">Machine learning</keyword><keyword weight="0.41847">Mathematics</keyword></keywords><publisher>JMLR.org</publisher><abstract>In this article we study the generalization abilities of several classifiers of support vector machine (SVM) type using a certain class of kernels that we call universal. It is shown that the soft margin algorithms with universal kernels are consistent for a large class of classification problems including some kind of noisy tasks provided that the regularization parameter is chosen well. In particular we derive a simple sufficient condition for this parameter in the case of Gaussian RBF kernels. On the one hand our considerations are based on an investigation of an approximation property---the so-called universality---of the used kernels that ensures that all continuous functions can be approximated by certain kernel expressions. This approximation property also gives a new insight into the role of kernels in these and other algorithms. On the other hand the results are achieved by a precise study of the underlying optimization problems of the classifiers. Furthermore, we show consistency for the maximal margin classifier as well as for the soft margin SVMu0027s in the presence of large margins. In this case it turns out that also constant regularization parameters ensure consistency for the soft margin SVMu0027s. Finally we prove that even for simple, noise free classification problems SVMu0027s with polynomial kernels can behave arbitrarily badly.</abstract></paper>