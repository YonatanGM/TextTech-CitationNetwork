<paper id="1982381767"><title>Toward efficient agnostic learning</title><year>1992</year><authors><author org="AT&amp;T Bell Lab, Murray Hill, NJ" id="2118586410">Michael J. Kearns</author><author org="AT&amp;T Bell Lab, Murray Hill, NJ" id="278177626">Robert E. Schapire</author><author org="Univ, of Chicago, Chicago, IL" id="240780293">Linda M. Sellie</author></authors><n_citation>315</n_citation><doc_type>Conference</doc_type><references><reference>1766442844</reference><reference>1858345456</reference><reference>1968998685</reference><reference>1993430291</reference><reference>2011039300</reference><reference>2019363670</reference><reference>2020246210</reference><reference>2042194938</reference><reference>2056957301</reference><reference>2084310470</reference><reference>2084544490</reference><reference>2087197805</reference><reference>2091401625</reference><reference>2117049614</reference><reference>2129192653</reference><reference>2138790649</reference><reference>2140259914</reference><reference>2142399242</reference><reference>2142471335</reference><reference>2154952480</reference><reference>2157526632</reference></references><venue id="1177622950" type="C">Conference on Learning Theory</venue><doi>10.1145/130385.130424</doi><keywords><keyword weight="0.60105">Algorithmic learning theory</keyword><keyword weight="0.58053">Semi-supervised learning</keyword><keyword weight="0.58899">Instance-based learning</keyword><keyword weight="0.58211">Stability (learning theory)</keyword><keyword weight="0.59816">Probably approximately correct learning</keyword><keyword weight="0.45293">Computer science</keyword><keyword weight="0.4578">Theoretical computer science</keyword><keyword weight="0.57767">Unsupervised learning</keyword><keyword weight="0.45796">Artificial intelligence</keyword><keyword weight="0.58328">Computational learning theory</keyword><keyword weight="0.45468">Machine learning</keyword><keyword weight="0.57875">Proactive learning</keyword></keywords><publisher>ACM</publisher><abstract>In this paper we initiate an investigation of generalizations of the Probably Approximately Correct (PAC) learning model that attempt to significantly weaken the target function assumptions. The ultimate goal in this direction is informally termed agnostic learning , in which we make virtually no assumptions on the target function. The name derives from the fact that as designers of learning algorithms, we give up the belief that Nature (as represented by the target function) has a simple or succinct explanation. We give a number of both positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an efficient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnostic learning, and an algorithm for learning in a model for problems involving hidden variables.</abstract></paper>