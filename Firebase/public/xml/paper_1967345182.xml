<paper id="1967345182"><title>The state of retrieval system evaluation</title><year>1992</year><authors><author org="Department of Computer Science, Cornell University, Ithaca, NY 14853-7501, U.S.A." id="2101230262">Gerard Salton</author></authors><n_citation>122</n_citation><doc_type>Journal</doc_type><references><reference>1956559956</reference><reference>1964801579</reference><reference>1978394996</reference><reference>1979346010</reference><reference>1986866538</reference><reference>1995461500</reference><reference>2000672666</reference><reference>2015547952</reference><reference>2039142665</reference><reference>2052989395</reference><reference>2054409659</reference><reference>2068632118</reference><reference>2076982227</reference><reference>2083605078</reference><reference>2095396650</reference></references><venue id="174847851" type="J">Information Processing and Management</venue><doi>10.1016/0306-4573(92)90002-H</doi><keywords><keyword weight="0.47199">Information retrieval</keyword><keyword weight="0.49521">Credibility</keyword><keyword weight="0.43933">Computer science</keyword><keyword weight="0.0">System evaluation</keyword><keyword weight="0.52866">Search engine indexing</keyword><keyword weight="0.66632">Relevance (information retrieval)</keyword><keyword weight="0.61938">Document retrieval</keyword><keyword weight="0.43249">Technical report</keyword></keywords><publisher>Cornell University</publisher><abstract>Abstract Substantial misgivings have been voiced over the years about the methodologies used to evaluate information retrieval procedures, and about the credibility of many of the available test results. In this note, an attempt is made to review the state of retrieval evaluation and to separate certain misgivings about the design of retrieval tests from conclusions that can legitimately be drawn from the evaluation results.</abstract></paper>