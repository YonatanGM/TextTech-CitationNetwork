<paper id="1605688901"><title>An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization</title><year>2000</year><authors><author org="Department of Computer Science, Oregon State University, Corvallis, OR 97331, USA. [email protected]#TAB#" id="160031478">Thomas G. Dietterich</author></authors><n_citation>1898</n_citation><doc_type>Journal</doc_type><references><reference>1562197959</reference><reference>1597165973</reference><reference>1850527962</reference><reference>1966280301</reference><reference>2042614373</reference><reference>2081869978</reference><reference>2112076978</reference><reference>2152761983</reference><reference>2167277498</reference><reference>2912934387</reference></references><venue id="62148650" type="J">Machine Learning</venue><doi>10.1023/A:1007607513941</doi><keywords><keyword weight="0.49888">Decision tree</keyword><keyword weight="0.43686">Data mining</keyword><keyword weight="0.59466">Ensembles of classifiers</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.56844">Random forest</keyword><keyword weight="0.55086">Ensemble learning</keyword><keyword weight="0.62147">BrownBoost</keyword><keyword weight="0.43382">Pattern recognition</keyword><keyword weight="0.58516">Boosting (machine learning)</keyword><keyword weight="0.56749">LPBoost</keyword><keyword weight="0.45146">Machine learning</keyword><keyword weight="0.37642">Mathematics</keyword><keyword weight="0.65542">Gradient boosting</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a “base” learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.</abstract></paper>