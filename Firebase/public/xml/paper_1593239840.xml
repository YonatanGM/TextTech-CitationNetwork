<paper id="1593239840"><title>Similarity-Based Models of Word Cooccurrence Probabilities</title><year>1999</year><authors><author org="Dept. of Mathematics and Computer Science, Bar Ilan University, Ramat Gan 52900, Israel. dagan@macs.biu.ac.il#TAB#" id="356610775">Ido Dagan</author><author org="Department of Computer Science, Cornell University, Ithaca, NY 14853, USA. llee@cs.cornell.edu#TAB#" id="2124867291">Lillian Lee</author><author org="AT&amp;T Labs—Research, 180 Park Ave., Florham Park, NJ 07932, USA. pereira@research.att.com#TAB#" id="2291088731">Fernando C. N. Pereira</author></authors><n_citation>319</n_citation><doc_type>Journal</doc_type><references><reference>47415966</reference><reference>1517267840</reference><reference>1608874027</reference><reference>1647729745</reference><reference>1680231578</reference><reference>1689445748</reference><reference>1715319291</reference><reference>1993258438</reference><reference>2004131797</reference><reference>2007107125</reference><reference>2016001305</reference><reference>2040004971</reference><reference>2058045861</reference><reference>2059849436</reference><reference>2095958485</reference><reference>2098750402</reference><reference>2099247782</reference><reference>2102997946</reference><reference>2113641473</reference><reference>2121227244</reference><reference>2122111042</reference><reference>2123084125</reference><reference>2123489126</reference><reference>2127314673</reference><reference>2127675721</reference><reference>2134237567</reference><reference>2140842551</reference><reference>2145269499</reference><reference>2146950091</reference><reference>2147169507</reference><reference>2151375725</reference><reference>2155063582</reference><reference>2157025692</reference><reference>2162151798</reference><reference>2295097532</reference><reference>2953332543</reference></references><venue id="62148650" type="J">Machine Learning</venue><doi>10.1023/A:1007537716579</doi><keywords><keyword weight="0.56113">Perplexity</keyword><keyword weight="0.60422">Cache language model</keyword><keyword weight="0.42228">Computer science</keyword><keyword weight="0.46666">Natural language processing</keyword><keyword weight="0.557">Bigram</keyword><keyword weight="0.60561">Word Association</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.60717">Language model</keyword><keyword weight="0.52885">Factored language model</keyword><keyword weight="0.61748">SemEval</keyword><keyword weight="0.44894">Pattern recognition</keyword><keyword weight="0.4591">Speech recognition</keyword><keyword weight="0.56799">Language identification</keyword><keyword weight="0.45166">Machine learning</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations “eat a peach” and ”eat a beach” is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on “most similar” :[106],"describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition :[170],"also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task.</abstract></paper>