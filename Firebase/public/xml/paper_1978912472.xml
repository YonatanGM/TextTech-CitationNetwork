<paper id="1978912472"><title>Probabilistic and Rule-Based Tagger of an Inflective Language- a Comparison</title><year>1997</year><authors><author org="Institute of Formal and Applied Linguistics, Prague#TAB#" id="261932608">Jan Hajic</author><author org="Institute of Formal and Applied Linguistics, Prague#TAB#" id="2149117298">Barbora Hladka</author></authors><n_citation>37</n_citation><doc_type>Conference</doc_type><references><reference>1632114991</reference><reference>2046224275</reference><reference>2112861996</reference><reference>2157693466</reference></references><venue id="2756391745" type="C">Conference on Applied Natural Language Processing</venue><doi>10.3115/974557.974574</doi><keywords><keyword weight="0.47899">Rule-based system</keyword><keyword weight="0.48479">Czech</keyword><keyword weight="0.51206">Trigram</keyword><keyword weight="0.41507">Computer science</keyword><keyword weight="0.47534">Word error rate</keyword><keyword weight="0.46719">Part of speech</keyword><keyword weight="0.4402">Speech recognition</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.51743">Bigram</keyword><keyword weight="0.45363">Natural language processing</keyword><keyword weight="0.48383">Probabilistic logic</keyword><keyword weight="0.49602">Tag system</keyword></keywords><publisher>Association for Computational Linguistics</publisher><abstract>We present results of probabilistic tagging of Czech texts in order to show how these techniques work for one of the highly morphologically ambiguous inflective languages. After description of the tag system used, we show the results of four experiments using a simple probabilistic model to tag Czech texts (unigram, two bigram experiments, and a trigram one). For comparison, we have applied the same code and settings to tag an English text (another four experiments) using the same size of training and test data in the experiments in order to avoid any doubt concerning the validity of the comparison. The experiments use the source channel model and maximum likelihood training on a Czech hand-tagged corpus and on tagged Wall Street Journal (WSJ) from the LDC collection. The experiments show (not surprisingly) that the more training data, the better is the success rate. The results also indicate that for inflective languages with 1000+ tags we have to develop a more sophisticated approach in order to get closer to an acceptable error rate. In order to compare two different approaches to text tagging---statistical and rule-based --- we modified Eric Brillu0027s rule-based part of speech tagger and carried out two more experiments on the Czech data, obtaining similar results in terms of the error rate. We have also run three more experiments with greatly reduced tagset to get another comparison based on similar tagset size.</abstract></paper>