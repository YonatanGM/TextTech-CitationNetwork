<paper id="1492405878"><title>Synchronization Issues in Data-Parallel Languages</title><year>1993</year><authors><author org="University of California" id="2118421755">Sundeep Prakash</author><author org="University of California" id="1220329227">Maneesh Dhagat</author><author org="University of California" id="2293596591">Rajive Bagrodia</author></authors><n_citation>13</n_citation><doc_type>Conference</doc_type><references><reference>1973310391</reference><reference>1976611694</reference><reference>2025139261</reference><reference>2033214895</reference><reference>2098688018</reference><reference>2134205263</reference><reference>2135736783</reference><reference>2140534901</reference><reference>2144344516</reference></references><venue id="1198129048" type="C">Languages and Compilers for Parallel Computing</venue><doi>10.1007/3-540-57659-2_5</doi><keywords><keyword weight="0.55368">Asynchronous communication</keyword><keyword weight="0.54992">Synchronization</keyword><keyword weight="0.426">Architecture</keyword><keyword weight="0.51122">Programmer</keyword><keyword weight="0.47087">Programming language</keyword><keyword weight="0.47682">Computer science</keyword><keyword weight="0.56006">Language construct</keyword><keyword weight="0.46415">Theoretical computer science</keyword><keyword weight="0.57362">Compiler</keyword><keyword weight="0.0">Execution time</keyword><keyword weight="0.43942">Granularity</keyword><keyword weight="0.47197">Distributed computing</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>Data-parallel programming has established itself as the preferred way of programming a large class of scientific applications. In this paper, we address the issue of reducing synchronization costs when implementing a data-parallel language on an asynchronous architecture. The synchronization issue is addressed from two perspectives: first, we describe language constructs that allow the programmer to specify that different parts of a data-parallel program be synchronized at different levels of granularity. Secondly, we show how existing tools and algorithms for data dependency analysis can be used by the compiler to both reduce the number of barriers and to replace global barriers by cheaper clustered synchronizations. Although the techniques presented in the paper are general purpose, we describe them in the context of a data-parallel language called UC developed at UCLA. Reducing the number of barriers improves program execution time by reducing synchronization time and also processor stall times.</abstract></paper>