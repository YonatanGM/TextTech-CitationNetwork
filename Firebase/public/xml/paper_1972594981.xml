<paper id="1972594981"><title>A study of smoothing methods for language models applied to information retrieval</title><year>2004</year><authors><author org="Carnegie Mellon University, Urbana, IL#TAB#" id="2152766206">Chengxiang Zhai</author><author org="Carnegie Mellon University, Pittsburgh Pa" id="1976640904">John Lafferty</author></authors><n_citation>996</n_citation><doc_type>Journal</doc_type><references><reference>1482214997</reference><reference>1540124269</reference><reference>1934041838</reference><reference>1964348731</reference><reference>1965282483</reference><reference>1978394996</reference><reference>1989468977</reference><reference>2000569744</reference><reference>2000672666</reference><reference>2026937586</reference><reference>2047031127</reference><reference>2062270497</reference><reference>2068905009</reference><reference>2075201173</reference><reference>2093390569</reference><reference>2095368471</reference><reference>2099437808</reference><reference>2100279283</reference><reference>2132957691</reference><reference>2134237567</reference><reference>2136583886</reference><reference>2136729221</reference><reference>2155520241</reference><reference>2165612380</reference><reference>2169213601</reference><reference>2911767655</reference></references><venue id="87067389" type="J">ACM Transactions on Information Systems</venue><doi>10.1145/984321.984322</doi><keywords><keyword weight="0.47685">Data mining</keyword><keyword weight="0.46366">Information retrieval</keyword><keyword weight="0.45328">Computer science</keyword><keyword weight="0.52645">Expectationâ€“maximization algorithm</keyword><keyword weight="0.0">Maximum likelihood</keyword><keyword weight="0.68944">Smoothing</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.50232">Test data</keyword><keyword weight="0.52854">Estimation theory</keyword><keyword weight="0.60394">Language model</keyword><keyword weight="0.47501">Machine learning</keyword></keywords><publisher>ACM</publisher><abstract>Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role---to make the estimated document language model more accurate and to "explain" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data.</abstract></paper>