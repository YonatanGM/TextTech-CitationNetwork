<paper id="1934041838"><title>Improved backing-off for M-gram language modeling</title><year>1995</year><authors><author org="Philips GmbH Forschungslab., Aachen, Germany" id="200456398">R. Kneser</author><author org="" id="2293758362">H. Ney</author></authors><n_citation>1147</n_citation><doc_type>Conference</doc_type><references><reference>2024490156</reference><reference>2075201173</reference><reference>2134237567</reference><reference>2158905201</reference></references><venue id="1121227772" type="C">International Conference on Acoustics, Speech, and Signal Processing</venue><doi>10.1109/ICASSP.1995.479394</doi><keywords><keyword weight="0.57043">Perplexity</keyword><keyword weight="0.44456">Pattern recognition</keyword><keyword weight="0.52753">Kneserâ€“Ney smoothing</keyword><keyword weight="0.42591">Computer science</keyword><keyword weight="0.51961">Word error rate</keyword><keyword weight="0.48611">Stochastic process</keyword><keyword weight="0.54634">Probability distribution</keyword><keyword weight="0.47578">Natural language</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.47313">Sparse matrix</keyword><keyword weight="0.51714">Language model</keyword></keywords><publisher>IEEE</publisher><abstract>In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10% in terms of perplexity and 5% in terms of word error rate.</abstract></paper>