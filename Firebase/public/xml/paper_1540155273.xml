<paper id="1540155273"><title>A Generalized Representer Theorem</title><year>2001</year><authors><author org="Microsoft Research Ltd" id="297432538">Bernhard Sch√∂lkopf</author><author org="Microsoft Research Ltd" id="1294330762">Ralf Herbrich</author><author org="Australian Nat. University" id="1972291593">Alex J. Smola</author></authors><n_citation>950</n_citation><doc_type>Conference</doc_type><references><reference>1480643256</reference><reference>1496612019</reference><reference>1587559447</reference><reference>1979711143</reference><reference>1981025032</reference><reference>2087347434</reference><reference>2110630246</reference><reference>2135845071</reference><reference>2149842772</reference><reference>2156909104</reference></references><venue id="2740279309" type="C">European Conference on Computational Learning Theory</venue><doi>10.1007/3-540-44581-1_27</doi><keywords><keyword weight="0.51433">Kernel (linear algebra)</keyword><keyword weight="0.56854">Hilbert space</keyword><keyword weight="0.45616">Discrete mathematics</keyword><keyword weight="0.50201">Feature vector</keyword><keyword weight="0.52227">Support vector machine</keyword><keyword weight="0.52299">Curse of dimensionality</keyword><keyword weight="0.52116">Kernel principal component analysis</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.70073">Representer theorem</keyword><keyword weight="0.5577">Kernel method</keyword><keyword weight="0.42619">Machine learning</keyword><keyword weight="0.42385">Mathematics</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>Wahbau0027s classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.</abstract></paper>