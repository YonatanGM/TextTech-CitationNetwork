<paper id="1573640198"><title>Deconstructing the Digit Recognition Problem</title><year>1992</year><authors><author org="Department of Computer Science, CUNY/Hunter College, 695 Park Avenue, New York, NY Â· 10021, 212-772-4283" id="2155410385">Cullen Schaffer</author></authors><n_citation>16</n_citation><doc_type>Conference</doc_type><references><reference>1602363634</reference><reference>1604329830</reference><reference>1638172072</reference><reference>1908447011</reference><reference>1983661866</reference><reference>1995023359</reference><reference>2128420091</reference></references><venue id="1180662882" type="C">International Conference on Machine Learning</venue><doi>10.1016/B978-1-55860-247-2.50056-5</doi><keywords><keyword weight="0.0">Training set</keyword><keyword weight="0.44848">ENCODE</keyword><keyword weight="0.45843">Pattern recognition</keyword><keyword weight="0.43821">Computer science</keyword><keyword weight="0.45941">Artificial intelligence</keyword><keyword weight="0.72196">Pruning (decision trees)</keyword><keyword weight="0.63822">Overfitting</keyword><keyword weight="0.0">Digit recognition</keyword><keyword weight="0.46615">Machine learning</keyword><keyword weight="0.48705">Test data generation</keyword><keyword weight="0.44367">Pruning</keyword></keywords><publisher>Morgan Kaufmann Publishers Inc.</publisher><abstract>Decision tree pruning techniques and other forms of overfitting avoidance have often been considered statistical means of improving predictive accuracy. Intuitively, they are intended to determine the appropriate level of complexity for an induced model by distinguishing between signal and noise in training data, patterns that reflect the true underlying nature of data generation and those that arise by chance. In fact, however, overfitting avoidance methods simply encode preferences for certain classes of models. These preferences may increase accuracy when they bias induction in favor of predictive models, but they may just as well have the opposite effect if they bias induction away from predictive models. This paper analyzes the conditional value of overfitting avoidance and focuses on the effect of one highly-regarded pruning technique in variations of the well-known digit recognition problem.</abstract></paper>