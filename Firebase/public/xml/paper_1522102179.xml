<paper id="1522102179"><title>Learning DNF via probabilistic evidence combination</title><year>1993</year><authors><author org="Department of Computer Science, Hill Center for the Mathematical Sciences, Busch Campus, Rutgers University, New Brunswick, NJ 08903, norton@cs.rutgers.edu, hirsh@cs.rutgers.edu" id="2199044432">Steven W. Norton</author><author org="Department of Computer Science, Hill Center for the Mathematical Sciences, Busch Campus, Rutgers University, New Brunswick, NJ 08903, norton@cs.rutgers.edu, hirsh@cs.rutgers.edu" id="2151078108">Haym Hirsh</author></authors><n_citation>10</n_citation><doc_type>Conference</doc_type><references><reference>77170824</reference><reference>146100937</reference><reference>1570286060</reference><reference>1570605737</reference><reference>1994022788</reference><reference>1999138184</reference><reference>2009207944</reference><reference>2089967664</reference><reference>2090559885</reference><reference>2107189314</reference><reference>2136000097</reference><reference>2428981601</reference><reference>2974365732</reference></references><venue id="1180662882" type="C">International Conference on Machine Learning</venue><doi>10.1016/B978-1-55860-307-3.50035-6</doi><keywords><keyword weight="0.0">Noisy data</keyword><keyword weight="0.48633">Expression (mathematics)</keyword><keyword weight="0.46189">Pattern recognition</keyword><keyword weight="0.44207">Computer science</keyword><keyword weight="0.49357">Synthetic data</keyword><keyword weight="0.46602">Artificial intelligence</keyword><keyword weight="0.0">Generalization error</keyword><keyword weight="0.50862">Probabilistic logic</keyword><keyword weight="0.42118">Disjunct</keyword><keyword weight="0.46783">Machine learning</keyword></keywords><publisher /><abstract>One approach to learning DNF expressions from examples is to use a conjunctive learner to separately form each of the disjuncts. This paper describes a learning algorithm that follows this approach, extending our earlier work on learning conjunctive classifiers from noisy data to learning DNF classifiers from noisy data. Because every disjunct does not cover every positive example, such learners must decide which positive examples to cover with each disjunct being learned. The central idea here is to model as representational noise the uncertainty as to whether a positive example should be treated as positive for a particular disjunct, in addition to whatever other noise may be imposed on the data by the environment. In experiments with synthetic data our learning method exhibits statistically significantly lower error rates during early learning when compared to the C4.5 algorithm, and on seven out of eight real-world datasets the error rates of classifiers learned by the new algorithm meet or exceed those of the classifiers learned by C4.5.</abstract></paper>