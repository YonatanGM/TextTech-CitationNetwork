<paper id="1552508833"><title>Understanding Probabilistic Classifiers</title><year>2001</year><authors><author org="Department of Computer Science and the Beckman Institute, University of Illinois, Urbana, IL#TAB#" id="2594661123">Ashutosh Garg</author><author org="Department of Computer Science and the Beckman Institute, University of Illinois, Urbana, IL#TAB#" id="2122007671">Dan Roth</author></authors><n_citation>51</n_citation><doc_type>Conference</doc_type><references><reference>1648417313</reference><reference>1774901127</reference><reference>1817561967</reference><reference>1840338487</reference><reference>2019363670</reference><reference>2099294566</reference><reference>2140785063</reference><reference>2155511848</reference></references><venue id="2755314191" type="C">European conference on Machine Learning</venue><doi>10.1007/3-540-44795-4_16</doi><keywords><keyword weight="0.44798">Data set</keyword><keyword weight="0.53273">Joint probability distribution</keyword><keyword weight="0.57728">Naive Bayes classifier</keyword><keyword weight="0.43547">Computer science</keyword><keyword weight="0.53897">Bayesian network</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.56248">Probabilistic logic</keyword><keyword weight="0.47587">Conditional entropy</keyword><keyword weight="0.48305">Classifier (linguistics)</keyword><keyword weight="0.51254">Marginal distribution</keyword><keyword weight="0.45298">Machine learning</keyword></keywords><publisher>Springer-Verlag</publisher><abstract>Probabilistic classifiers are developed by assuming generative models which are product distributions over the original attribute space (as in naive Bayes) or more involved spaces (as in general Bayesian networks). While this paradigm has been shown experimentally successful on real world applications, despite vastly simplified probabilistic assumptions, the question of why these approaches work is still :[56],"paper resolves this question.We show that almost all joint distributions with a given set of marginals (i.e., all distributions that could have given rise to the classifier learned) or, equivalently, almost all data sets that yield this set of marginals, are very close (in terms of distributional distance) to the product distribution on the marginals; the number of these distributions goes down exponentially with their distance from the product distribution. Consequently, as we show, for almost all joint distributions with this set of marginals, the penalty incurred in using the marginal distribution rather than the true one is small. In addition to resolving the puzzle surrounding the success of probabilistic classifiers our results contribute to understanding the tradeoffs in developing probabilistic classifiers and will help in developing better classifiers.</abstract></paper>