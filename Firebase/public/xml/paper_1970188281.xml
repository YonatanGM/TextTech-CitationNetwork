<paper id="1970188281"><title>Further results on the margin distribution</title><year>1999</year><authors><author org="Department of Computer Science, Royal Holloway, University of London, Egham, Surrey TW20 0EX, UK" id="2215287472">John Shawe-Taylor</author><author org="Dept of Engineering Mathematics, University of Bristol, Bristol BS8 1TR, UK" id="156417696">Nello Cristianini</author></authors><n_citation>60</n_citation><doc_type>Conference</doc_type><references><reference>1500284251</reference><reference>1553313034</reference><reference>1570871893</reference><reference>1574111330</reference><reference>1975846642</reference><reference>1979711143</reference><reference>2017753243</reference><reference>2040293685</reference><reference>2098774896</reference><reference>2099579348</reference><reference>2106491486</reference><reference>2119821739</reference></references><venue id="1177622950" type="C">Conference on Learning Theory</venue><doi>10.1145/307400.307470</doi><keywords><keyword weight="0.50166">Least squares</keyword><keyword weight="0.51495">Slack variable</keyword><keyword weight="0.62224">Margin (machine learning)</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.63094">Margin classifier</keyword><keyword weight="0.54128">Perceptron</keyword><keyword weight="0.48451">Sample size determination</keyword><keyword weight="0.43293">Mathematics</keyword><keyword weight="0.41571">Machine learning</keyword><keyword weight="0.49891">Special case</keyword><keyword weight="0.54319">Bounded function</keyword></keywords><publisher>ACM</publisher><abstract>A number of results have bounded generalization error of a classifier in terms of its margin on the training points. There has been some debate about whether the minimum margin is the best measure of the distribution of training set margin values with which to estimate the generalization error. Freund and Schapire [7] have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as an expected error bound. Shawe-Taylor and Cristianini [ 131 showed that a slight generalization of their construction can be used to give a pat style bound on the tail of the distribution of the generalization errors that arise from a given sample size when using threshold linear classifiers. We show that in the linear case the approach can be viewed as a change of kernel and that the algorithms arising from the approach are exactly those originally proposed by Cortes and Vapnik [4]. We generalise the basic result to function classes with bounded fat-shattering dimension and the Ii measure for slack variables which gives rise to Vapnikâ€™s box constraint algorithm. Finally, application to regression is considered, which includes standard least squares as a special case.</abstract></paper>