<paper id="1971281888"><title>Vision texture for annotation</title><year>1995</year><authors><author org="Vision and Modeling Group, MIT Media Laboratory, Cambridge, USA" id="687641213">R. W. Picard</author><author org="Vision and Modeling Group, MIT Media Laboratory, Cambridge, USA" id="172536002">T. P. Minka</author></authors><n_citation>235</n_citation><doc_type>Journal</doc_type><references><reference>13726599</reference><reference>1541987076</reference><reference>1971784203</reference><reference>1988445395</reference><reference>2021751319</reference><reference>2023952725</reference><reference>2093191240</reference><reference>2096111861</reference><reference>2096265622</reference><reference>2105994593</reference><reference>2127370936</reference><reference>2137542411</reference><reference>2168977926</reference><reference>2571821494</reference></references><venue id="112262039" type="J">Multimedia Systems</venue><doi>10.1007/BF01236575</doi><keywords><keyword weight="0.46446">Computer vision</keyword><keyword weight="0.0">Texture model</keyword><keyword weight="0.47424">Annotation</keyword><keyword weight="0.4685">Information retrieval</keyword><keyword weight="0.62292">Image texture</keyword><keyword weight="0.44834">Computer science</keyword><keyword weight="0.50516">Search engine indexing</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.46802">Digital library</keyword><keyword weight="0.45506">Perception</keyword><keyword weight="0.44987">Multimedia</keyword></keywords><publisher>Springer-Verlag New York, Inc.</publisher><abstract>This paper demonstrates a new application of computer vision to digital libraries — the use of texture forannotation, the description of content. Vision-based annotation assists the user in attaching descriptions to large sets of images and video. If a user labels a piece of an image aswater, a texture model can be used to propagate this label to other “visually similar” regions. However, a serious problem is that no single model has been found that is good enough to match reliably human perception of similarity in pictures. Rather than using one model, the system described here knows several texture models, and is equipped with the ability to choose the one that “best explains” the regions selected by the user for annotating. If none of these models suffices, then it creates new explanations by combining models. Examples of annotations propagated by the system on natural scenes are given. The system provides an average gain of four to one in label prediction for a set of 98 images.</abstract></paper>