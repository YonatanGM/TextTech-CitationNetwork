<paper id="1481802362"><title>Improved training via incremental learning</title><year>1989</year><authors><author org="Department of Computer and Information Science, University of Massachusetts, Amherst, MA" id="2240102910">Paul E. Utgoff</author></authors><n_citation>23</n_citation><doc_type>Conference</doc_type><references><reference>167515793</reference><reference>206794860</reference><reference>1490514105</reference><reference>1573772481</reference><reference>2149706766</reference></references><venue id="1180662882" type="C">International Conference on Machine Learning</venue><doi>10.1016/B978-1-55860-036-2.50092-8</doi><keywords><keyword weight="0.61068">Decision tree</keyword><keyword weight="0.68018">Instance-based learning</keyword><keyword weight="0.44427">Pattern recognition</keyword><keyword weight="0.45674">Computer science</keyword><keyword weight="0.0">Incremental learning</keyword><keyword weight="0.0">Instance selection</keyword><keyword weight="0.47396">Artificial intelligence</keyword><keyword weight="0.46605">ID3</keyword><keyword weight="0.47321">Machine learning</keyword><keyword weight="0.77058">Incremental decision tree</keyword></keywords><publisher>Morgan Kaufmann Publishers Inc.</publisher><abstract>It is well known that it is possible to choose training instances more carefully when using incremental learning than when using nonincremental learning. This is because a partially learned concept can provide guidance about which instances would be informative training instances. The point is illustrated by comparing the incremental decision tree induction algorithm ID5R with ID3 on Quinlanu0027s chess task. The instance selection strategy for ID5R finds a set of training instances that produces the smallest decision tree yet found for the chess task.</abstract></paper>