<paper id="1978626680"><title>Back-propagation algorithm which varies the number of hidden units</title><year>1991</year><authors><author org="Fujitsu Laboratories Ltd (Japan)" id="2112132790">Yoshio Hirose</author><author org="Fujitsu Laboratories Ltd (Japan)" id="2936973654">Koichi Yamashita</author><author org="Fujitsu Laboratories Ltd (Japan)" id="2677966594">Shimpei Hijiya</author></authors><n_citation>435</n_citation><doc_type>Journal</doc_type><references><reference>1580142630</reference></references><venue id="123019304" type="J">Neural Networks</venue><doi>10.1016/0893-6080(91)90032-Z</doi><keywords><keyword weight="0.53496">Alphanumeric</keyword><keyword weight="0.0">Back propagation algorithm</keyword><keyword weight="0.45735">Font</keyword><keyword weight="0.45974">Algorithm</keyword><keyword weight="0.51881">Maxima and minima</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.54115">Artificial neural network</keyword><keyword weight="0.51297">Backpropagation</keyword><keyword weight="0.43756">Machine learning</keyword><keyword weight="0.39082">Mathematics</keyword></keywords><publisher>Elsevier Science Ltd.</publisher><abstract>This report presents a back-propagation algorithm that varies the number of hidden units. This algorithm is expected to escape local minima and makes it no longer necessary to decide the number of hidden units. We tested this algorithm on two examples. One was exclusive-OR learning and the other was 8 Ã— 8 dot alphanumeric font learning. In both examples, the probability of becoming trapped in local minima was reduced. Furthermore, in alphanumeric font learning, the network converged two to three times faster than conventional back-propagation.</abstract></paper>