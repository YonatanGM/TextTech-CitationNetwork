<paper id="1529840045"><title>Online bagging and boosting</title><year>2005</year><authors><author org="Intelligent Syst. Div., NASA Ames Res. Center, Moffett Field, CA, USA" id="2579304871">N.C. Oza</author></authors><n_citation>479</n_citation><doc_type>Conference</doc_type><references><reference>1988790447</reference><reference>2082168306</reference><reference>2150407757</reference><reference>2155714768</reference><reference>2400267228</reference><reference>2491694318</reference><reference>2912934387</reference></references><venue id="1170695740" type="C">Systems, Man and Cybernetics</venue><doi>10.1109/ICSMC.2005.1571498</doi><keywords><keyword weight="0.55453">Online machine learning</keyword><keyword weight="0.45369">Data mining</keyword><keyword weight="0.50548">Semi-supervised learning</keyword><keyword weight="0.44094">Computer science</keyword><keyword weight="0.57673">Boosting (machine learning)</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.48762">Batch processing</keyword><keyword weight="0.51984">Computational learning theory</keyword><keyword weight="0.54408">Ensemble learning</keyword><keyword weight="0.46189">Machine learning</keyword><keyword weight="0.59922">BrownBoost</keyword><keyword weight="0.48629">Random access</keyword></keywords><publisher>IEEE</publisher><abstract>Bagging and boosting are two of the most well-known ensemble learning methods due to their theoretical performance guarantees and strong experimental results. However, these algorithms have been used mainly in batch mode, i.e., they require the entire training set to be available at once and, in some cases, require random access to the data. In this paper, we present online versions of bagging and boosting that require only one pass through the training data. We build on previously presented work by describing some theoretical results. We also compare the online and batch algorithms experimentally in terms of accuracy and running time.</abstract></paper>