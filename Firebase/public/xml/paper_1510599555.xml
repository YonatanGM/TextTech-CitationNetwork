<paper id="1510599555"><title>Compression, Significance, and Accuracy</title><year>1992</year><authors><author org="The Turing Institute, 36 North Hanover Street, Glasgow G1 2AD, UK" id="735181462">Stephen Muggleton</author><author org="The Turing Institute, 36 North Hanover Street, Glasgow G1 2AD, UK" id="2270196177">Ashwin Srinivasan</author><author org="The Turing Institute, 36 North Hanover Street, Glasgow G1 2AD, UK" id="1948433388">Michael Bain</author></authors><n_citation>52</n_citation><doc_type>Conference</doc_type><references><reference>78695714</reference><reference>1481042511</reference><reference>1565236324</reference><reference>1580451812</reference><reference>1969005071</reference><reference>1983661866</reference><reference>1999138184</reference><reference>2019363670</reference><reference>2054658115</reference><reference>2107189314</reference><reference>2131882306</reference><reference>2135625884</reference><reference>2136000097</reference></references><venue id="1180662882" type="C">International Conference on Machine Learning</venue><doi>10.1016/B978-1-55860-247-2.50048-6</doi><keywords><keyword weight="0.6081">Inductive logic programming</keyword><keyword weight="0.3702">Compression (physics)</keyword><keyword weight="0.44975">Computer science</keyword><keyword weight="0.50004">Truth value</keyword><keyword weight="0.47052">Input/output</keyword><keyword weight="0.51901">Turing machine</keyword><keyword weight="0.48791">Mathematical proof</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.45951">Machine learning</keyword><keyword weight="0.52086">Decision tree learning</keyword></keywords><publisher>Morgan Kaufmann Publishers Inc.</publisher><abstract>Inductive Logic Programming (ILP) involves learning relational concepts from examples and background knowledge. To date all ILP learning systems make use of tests inherited from propositional and decision tree learning for evaluating the significance of hypotheses. None of these significance tests take account of the relevance or utility of the background knowledge. In this paper we describe a method, called HP-compression, of evaluating the significance of a hypothesis based on the degree to which it allows compression of the observed data with respect to the background knowledge. This can be measured by comparing the lengths of the input and output tapes of a reference Turing machine which will generate the examples from the hypothesis and a set of derivational proofs. The model extends an earlier approach of Muggleton by allowing for noise. The truth values of noisy instances are switched by making use of correction codes. The utility of compression as a significance measure is evaluated empirically in three independent domains. In particular, the results show that the existence of positive compression distinguishes a larger number of significant clauses than other significance tests The method is also shown to reliably distinguish artificially introduced noise as incompressible data.</abstract></paper>