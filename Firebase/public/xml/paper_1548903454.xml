<paper id="1548903454"><title>A Sequential Approximation Bound for Some Sample-Dependent Convex Optimization Problems with Applications in Learning</title><year>2001</year><authors><author org="IBM T.J. Watson research center" id="2510858842">Tong Zhang</author></authors><n_citation>2</n_citation><doc_type>Conference</doc_type><references><reference>1485026097</reference><reference>1709057194</reference><reference>1968444040</reference><reference>2011395874</reference><reference>2043800867</reference><reference>2069317438</reference><reference>2077008409</reference><reference>2122789612</reference><reference>2138263042</reference><reference>2156909104</reference><reference>2408196097</reference><reference>2795056610</reference></references><venue id="2740279309" type="C">European Conference on Computational Learning Theory</venue><doi>10.1007/3-540-44581-1_5</doi><keywords><keyword weight="0.54557">Gradient method</keyword><keyword weight="0.50344">Online algorithm</keyword><keyword weight="0.59038">Online machine learning</keyword><keyword weight="0.52539">Gradient descent</keyword><keyword weight="0.48212">Batch production</keyword><keyword weight="0.46898">Mathematical optimization</keyword><keyword weight="0.48675">Regret</keyword><keyword weight="0.43023">Computer science</keyword><keyword weight="0.57762">Proximal gradient methods for learning</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.58991">Convex optimization</keyword><keyword weight="0.43681">Machine learning</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>In this paper, we study a class of sample dependent convex optimization problems, and derive a general sequential approximation bound for their solutions. This analysis is closely related to the regret bound framework in online learning. However we apply it to batch learning algorithms instead of online stochastic gradient decent methods. Applications of this analysis in some classification and regression problems will be illustrated.</abstract></paper>