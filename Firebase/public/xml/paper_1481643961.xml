<paper id="1481643961"><title>AdaBoosting Neural Networks: Application to on-line Character Recognition</title><year>1997</year><authors><author org="University of Montreal" id="2160549261">Holger Schwenk</author><author org="AT&amp;T Bell Laboratories#TAB#" id="161269817">Yoshua Bengio</author></authors><n_citation>24</n_citation><doc_type>Conference</doc_type><references><reference>1966280301</reference><reference>1975846642</reference><reference>1980652844</reference><reference>1988790447</reference><reference>2104364170</reference><reference>2112076978</reference><reference>2120486861</reference><reference>2137291015</reference></references><venue id="1158833223" type="C">International Conference on Artificial Neural Networks</venue><doi>10.1007/BFb0020278</doi><keywords><keyword weight="0.51558">Decision tree</keyword><keyword weight="0.65052">AdaBoost</keyword><keyword weight="0.61451">Stability (learning theory)</keyword><keyword weight="0.0">Character recognition</keyword><keyword weight="0.44951">Computer science</keyword><keyword weight="0.40568">Writing style</keyword><keyword weight="0.65013">Boosting (machine learning)</keyword><keyword weight="0.46818">Artificial intelligence</keyword><keyword weight="0.57499">Artificial neural network</keyword><keyword weight="0.47255">Machine learning</keyword><keyword weight="0.64285">BrownBoost</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>“Boosting” is a general method for improving the performance of any weak learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [4]. It has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [3], in particular decision trees [1,2,5]. In this paper we use AdaBoost to improve the performances of a strong learning algorithm: a neural network based on-line character recognition system. In particular we will show that it can be used to learn automatically a great variety of writing styles even when the amount of training data for each style varies a lot. Our system achieves about 1.4 % error on a handwritten digit data base of more than 200 writers.</abstract></paper>