<paper id="1672197616"><title>When Is ''Nearest Neighbor'' Meaningful?</title><year>1999</year><authors><author org="University of Wisconsin-Madison" id="2952954738">Kevin S. Beyer</author><author org="University of Wisconsin-Madison" id="2618230056">Jonathan Goldstein</author><author org="University of Wisconsin-Madison" id="2164040783">Raghu Ramakrishnan</author><author org="University of Wisconsin-Madison" id="200124073">Uri Shaft</author></authors><n_citation>1614</n_citation><doc_type>Conference</doc_type><references><reference>1488681765</reference><reference>1499049447</reference><reference>1559016707</reference><reference>1971927246</reference><reference>1975830550</reference><reference>1998512964</reference><reference>2000830496</reference><reference>2012352340</reference><reference>2014368637</reference><reference>2024766881</reference><reference>2049565975</reference><reference>2054740631</reference><reference>2104128006</reference><reference>2107779728</reference><reference>2123977795</reference><reference>2125148312</reference><reference>2129938590</reference><reference>2136006972</reference><reference>2138313032</reference><reference>2145725688</reference><reference>2155776210</reference><reference>2169351022</reference><reference>2571821494</reference><reference>2914885528</reference></references><venue id="1165285842" type="C">International Conference on Database Theory</venue><doi>10.1007/3-540-49257-7_15</doi><keywords><keyword weight="0.54921">k-nearest neighbors algorithm</keyword><keyword weight="0.45046">Data mining</keyword><keyword weight="0.43142">Indexation</keyword><keyword weight="0.0">Linear Scan</keyword><keyword weight="0.42932">Computer science</keyword><keyword weight="0.47992">Search engine indexing</keyword><keyword weight="0.54973">Curse of dimensionality</keyword><keyword weight="0.48469">Independent and identically distributed random variables</keyword><keyword weight="0.0">Synthetic data sets</keyword><keyword weight="0.61886">Nearest neighbor search</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>We explore the effect of dimensionality on the "nearest neighbor" problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10-15 dimensions. results :[75],"should not be interpreted to mean that high-dimensional indexing is never meaningful; we illustrate this point by identifying some high-dimensional workloads for which this effect does not occur. However, our results do emphasize that the methodology used almost universally in the database literature to evaluate high-dimensional indexing techniques is flawed, and :[75],"should be modified. In particular, most such techniques proposed in the literature are not evaluated versus simple linear scan, and are evaluated over workloads for which nearest neighbor is not meaningful. Often, even the reported experiments, when analyzed carefully, show that linear scan would outperform the techniques being proposed on the workloads studied in high (10-15) dimensionality!</abstract></paper>