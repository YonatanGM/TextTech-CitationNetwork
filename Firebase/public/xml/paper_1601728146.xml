<paper id="1601728146"><title>Prepositional Phrase Attachment Through a Backed-off Model</title><year>1999</year><authors><author org="" id="2170865064">Michael Collins</author><author org="" id="2236112217">James Brooks</author></authors><n_citation>151</n_citation><doc_type>Conference</doc_type><references><reference>1632114991</reference><reference>1859173823</reference><reference>2015042937</reference><reference>2029768569</reference><reference>2033586051</reference><reference>2076002267</reference><reference>2134237567</reference></references><venue id="1188739475" type="C">Meeting of the Association for Computational Linguistics</venue><doi>10.1007/978-94-017-2390-9_11</doi><keywords><keyword weight="0.60475">Noun phrase</keyword><keyword weight="0.50761">Verb</keyword><keyword weight="0.56302">Determiner phrase</keyword><keyword weight="0.51877">Head-marking language</keyword><keyword weight="0.39934">Computer science</keyword><keyword weight="0.59208">Phrase</keyword><keyword weight="0.426">Speech recognition</keyword><keyword weight="0.43844">Natural language processing</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.5798">Verb phrase ellipsis</keyword><keyword weight="0.4963">Ambiguity</keyword><keyword weight="0.54188">Language model</keyword></keywords><publisher>Springer Netherlands</publisher><abstract>Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity. Typically, ambiguous verb phrases of the form v np1 p np2 are resolved through a model which considers values of the four head words (v, n1, p and n2). This paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable. Results on Wall Street Journal data of 84.5% accuracy are obtained using this method. A surprising result is the importance of low-count events â€” ignoring events which occur less than 5 times in training data reduces performance to 81.6%.</abstract></paper>