<paper id="1601912440"><title>Discriminative Training of Gaussian Mixtures for Image Object Recognition</title><year>1999</year><authors><author org="RWTH Aachen - University of Technology#TAB#" id="2123791372">Jörg Dahmen</author><author org="RWTH Aachen - University of Technology#TAB#" id="1975576064">R. Schlüter</author><author org="RWTH Aachen - University of Technology#TAB#" id="2293758362">Hermann Ney</author></authors><n_citation>19</n_citation><doc_type /><references><reference>854322902</reference><reference>1843569938</reference><reference>2108072092</reference><reference>2121649666</reference><reference>2137291015</reference><reference>2156909104</reference><reference>2158275940</reference><reference>2166501286</reference></references><venue id="" type="">Mustererkennung 1999, 21. DAGM-Symposium</venue><doi>10.1007/978-3-642-60243-6_24</doi><keywords><keyword weight="0.46808">Pattern recognition</keyword><keyword weight="0.41042">Computer science</keyword><keyword weight="0.0">Maximum likelihood</keyword><keyword weight="0.46013">Speech recognition</keyword><keyword weight="0.47933">Gaussian</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.0">Class separability</keyword><keyword weight="0.58186">Discriminative model</keyword><keyword weight="0.51383">Cognitive neuroscience of visual object recognition</keyword></keywords><publisher>Springer Berlin Heidelberg</publisher><abstract>In this paper we present a discriminative training procedure for Gaussian mixture densities. Conventional maximum likelihood (ML) training of such mixtures proved to be very efficient for object recognition, even though each class is treated separately in training. Discriminative criteria offer the advantage that they also use out-of-class data, that is they aim at optimizing class separability. We present results on the US Postal Service (USPS) handwritten digits database and compare the discriminative results to those obtained by ML training. We also compare our best results with those reported by other groups, proving them to be state-of-the-art.</abstract></paper>