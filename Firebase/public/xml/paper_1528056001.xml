<paper id="1528056001"><title>Factorial Hidden Markov Models</title><year>1995</year><authors><author org="Department of Computer Science, University of Toronto, Toronto, ON M5S 3H5, Canada. E-mail: zoubin@cs.toronto.edu#TAB#" id="617670330">Zoubin Ghahramani</author><author org="Department of Brain &amp; Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA 02139, USA. E-mail: jordan@psyche.mit.edu#TAB#" id="2435751034">Michael I. Jordan</author></authors><n_citation>1023</n_citation><doc_type>Conference</doc_type><references><reference>1520448186</reference><reference>1578685895</reference><reference>1991133427</reference><reference>2025653905</reference><reference>2076118331</reference><reference>2083380015</reference><reference>2102409316</reference><reference>2104163628</reference><reference>2120603682</reference><reference>2145211911</reference><reference>2147496287</reference><reference>2151457493</reference><reference>2159080219</reference><reference>2161335636</reference><reference>2161523118</reference><reference>2162995740</reference><reference>2172116616</reference></references><venue id="1127325140" type="C">Neural Information Processing Systems</venue><doi>10.1023/A:1007425814087</doi><keywords><keyword weight="0.59135">Forward–backward algorithm</keyword><keyword weight="0.45097">Pattern recognition</keyword><keyword weight="0.54581">Exact algorithm</keyword><keyword weight="0.43381">Computer science</keyword><keyword weight="0.57623">Approximate inference</keyword><keyword weight="0.54367">Bayesian network</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.57498">Graphical model</keyword><keyword weight="0.58331">Hidden Markov model</keyword><keyword weight="0.45202">Machine learning</keyword><keyword weight="0.582">Gibbs sampling</keyword><keyword weight="0.67302">Hidden semi-Markov model</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>Hidden Markov models (HMMs) have proven to be one of the most widely used tools for learning probabilistic models of time series data. In an HMM, information about the past is conveyed through a single discrete variable—the hidden state. We discuss a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. We describe an exact algorithm for inferring the posterior probabilities of the hidden state variables given the observations, and relate it to the forward–backward algorithm for HMMs and to algorithms for more general graphical models. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or variational methods. Within the variational framework, we present a structured approximation in which the the state variables are decoupled, yielding a tractable algorithm for learning the parameters of the model. Empirical comparisons suggest that these approximations are efficient and provide accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach‘s chorales and show that factorial HMMs can capture statistical structure in this data set which an unconstrained HMM cannot.</abstract></paper>