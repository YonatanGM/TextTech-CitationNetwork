<paper id="1986199755"><title>Incremental conditioning of lower and upper probabilities</title><year>1995</year><authors><author org="Carnegie-Mellon Univ. Pittsburgh, Pennsylvania, USA" id="2139452564">Lonnie Chrisman</author></authors><n_citation>27</n_citation><doc_type>Journal</doc_type><references><reference>73484287</reference><reference>1512921847</reference><reference>1591335969</reference><reference>1854466897</reference><reference>1880908097</reference><reference>1967361629</reference><reference>1968736254</reference><reference>1987949663</reference><reference>1990386132</reference><reference>1993075287</reference><reference>1999900601</reference><reference>2012176089</reference><reference>2015779132</reference><reference>2015924135</reference><reference>2026332415</reference><reference>2050036885</reference><reference>2056687315</reference><reference>2063200894</reference><reference>2065504396</reference><reference>2066843515</reference><reference>2071673873</reference><reference>2073512429</reference><reference>2107091259</reference><reference>2112262428</reference><reference>2114952291</reference><reference>2116816533</reference><reference>2147565737</reference><reference>2158470251</reference><reference>2159080219</reference><reference>2162543357</reference><reference>2164296059</reference><reference>2178747745</reference><reference>2911561062</reference></references><venue id="33368595" type="J">International Journal of Approximate Reasoning</venue><doi>10.1016/0888-613X(94)00018-X</doi><keywords><keyword weight="0.44923">Discrete mathematics</keyword><keyword weight="0.43717">Noncommutative geometry</keyword><keyword weight="0.6001">Probability box</keyword><keyword weight="0.46589">Exponential function</keyword><keyword weight="0.0">Current distribution</keyword><keyword weight="0.45198">Algorithm</keyword><keyword weight="0.48439">Conditioning</keyword><keyword weight="0.58863">Probability distribution</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.41672">Machine learning</keyword><keyword weight="0.41733">Mathematics</keyword></keywords><publisher>Elsevier</publisher><abstract>Abstract Bayesian-style conditioning of an exact probability distribution can be done incrementally by updating the current distribution each time a new item of evidence is obtained. Many have suggested the use of lower and upper probabilities for representing bounds on probability distributions, which naturally suggests an analogous procedure of incremental conditioning using forms of interval arithemetic. Unfortunately, conditioning of lower and upper probability bounds loses information, yielding incorrect bounds when updates and performed incrementally and making the conditioning operation noncommutative. Furthermore, when lower probability functions are represented by way of their Mobius transforms, the operation of conditioning can cause an exponential explosion in the number of nonzero Mobius assignments used to represent the function. This paper presents an alternative representation for lower probability that overcomes these problems. By representing the results of both Dempster conditioning and strong consitioning, the representation indirectly encodes lower probability bounds in a form that allows updates to be performed incrementally without a loss of information. Conditioning with the new representation does not depend on the order of updates or on whether evidence is incorporated incrementally or all at once. The bounds obtained are exact when the original lower probabilities satisfy a property called 2-monotonicity. Although the new representation encodes more information about probability bounds than the straight representation, updates on the new representation never increase the number of Mobius assignments used to encode the lower probabilityâ€”a considerable improvement over the worst-case exponential increase seen with the straight representation. The new representation helps to improve the efficiency and convenience of representing and manipulating lower probabilities.</abstract></paper>