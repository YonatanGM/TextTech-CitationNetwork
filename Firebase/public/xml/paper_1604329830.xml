<paper id="1604329830"><title>An Empirical Comparison of Pruning Methods for Decision Tree Induction</title><year>1989</year><authors><author org="School of Industrial and Business Studies, University of Warwick, Coventry CV4 7AL, England. BSRCD@CU.WARWICK.AC.UK#TAB#" id="1991719720">John Mingers</author></authors><n_citation>492</n_citation><doc_type>Journal</doc_type><references><reference>78121749</reference><reference>177590838</reference><reference>1967403127</reference><reference>1999011285</reference><reference>2006864328</reference><reference>2128420091</reference><reference>2149706766</reference></references><venue id="62148650" type="J">Machine Learning</venue><doi>10.1023/A:1022604100933</doi><keywords><keyword weight="0.5646">Decision tree</keyword><keyword weight="0.45157">Data mining</keyword><keyword weight="0.43453">Computer science</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.5867">ID3 algorithm</keyword><keyword weight="0.56599">Alternating decision tree</keyword><keyword weight="0.44136">Pattern recognition</keyword><keyword weight="0.6284">Principal variation search</keyword><keyword weight="0.59808">Decision tree model</keyword><keyword weight="0.69738">Pruning (decision trees)</keyword><keyword weight="0.45572">Machine learning</keyword><keyword weight="0.58875">Decision tree learning</keyword><keyword weight="0.64967">Incremental decision tree</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>This paper compares five methods for pruning decision trees, developed from sets of examples. When used with uncertain rather than deterministic data, decision-tree induction involves three main stages—creating a complete tree able to classify all the training examples, pruning this tree to give statistical reliability, and processing the pruned tree to improve understandability. This paper concerns the second stage—pruning. It presents empirical comparisons of the five methods across several domains. The results show that three methods—critical value, error complexity and reduced error—perform well, while the other two may cause problems. They also show that there is no significant interaction between the creation and pruning methods.</abstract></paper>