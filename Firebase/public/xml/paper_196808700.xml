<paper id="196808700"><title>A neural network construction algorithm which maximizes the likelihood function</title><year>1995</year><authors><author org="" id="2028249872">Rudy Setiono</author></authors><n_citation>26</n_citation><doc_type>Journal</doc_type><references><reference>2006544565</reference><reference>2109779438</reference><reference>2128033389</reference><reference>2151058089</reference></references><venue id="159543560" type="J">Connection Science</venue><doi>10.1080/09540099550039327</doi><keywords><keyword weight="0.54387">Parity bit</keyword><keyword weight="0.45785">Mathematical optimization</keyword><keyword weight="0.49191">Quasi-Newton method</keyword><keyword weight="0.59671">Feedforward neural network</keyword><keyword weight="0.40147">Spiral</keyword><keyword weight="0.54897">Likelihood function</keyword><keyword weight="0.43279">Computer science</keyword><keyword weight="0.46697">Algorithm</keyword><keyword weight="0.50992">Mean squared error</keyword><keyword weight="0.52634">Artificial neural network</keyword><keyword weight="0.47846">Feed forward</keyword></keywords><publisher>Taylor &amp; Francis Group</publisher><abstract>A new method for constructing a feedforward neural network is proposed. The method starts with a single hidden unit and more units are added to the hidden layer one at a time until a network that completely recognizes all its input patterns is constructed. The novel idea about this method is that the network is trained to maximize a certain likelihood function and not to minimize the more widely used mean squared error function. We show that when a new hidden unit is added to the network, this likelihood function is guaranteed to increase and this increase ensures the finite termination of the method. We also provide a wide range of numerical results. The method was tested on the n -bit parity problems and the spiral problem. It was able to construct networks having less than n hidden units that solve the n -bit parity problems for n = 4, 5, 6, 7 and 8. The method was also tested on some real-world data and the networks it constructed were shown to be able to predict patterns not in the training set with ...</abstract></paper>