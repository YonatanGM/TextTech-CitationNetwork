<paper id="1503605645"><title>Pruning decision trees with misclassification costs</title><year>1998</year><authors><author org="School of Electrical Engineering, Purdue University, West Lafayette, IN#TAB#" id="2475505892">Jeffrey P. Bradford</author><author org="Data Mining and Visualization, Silicon Graphics, Inc., Mountain View, CA#TAB#" id="2072683867">Clayton Kunz</author><author org="Data Mining and Visualization, Silicon Graphics, Inc., Mountain View, CA#TAB#" id="73615348">Ron Kohavi</author><author org="Data Mining and Visualization, Silicon Graphics, Inc., Mountain View, CA#TAB#" id="2630585023">Clifford Brunk</author><author org="School of Electrical Engineering, Purdue University, West Lafayette, IN#TAB#" id="1994240001">Carla E. Brodley</author></authors><n_citation>146</n_citation><doc_type>Conference</doc_type><references><reference>85229681</reference><reference>203696055</reference><reference>1507034068</reference><reference>1512628935</reference><reference>1516193414</reference><reference>1590476178</reference><reference>1595468493</reference><reference>1598696986</reference><reference>1605695115</reference><reference>1864487875</reference><reference>1955600018</reference><reference>1983661866</reference><reference>2125055259</reference><reference>2128128566</reference><reference>2128420091</reference><reference>2146935111</reference><reference>2170913656</reference></references><venue id="2755314191" type="C">European conference on Machine Learning</venue><doi>10.1007/BFb0026682</doi><keywords><keyword weight="0.51462">Decision tree</keyword><keyword weight="0.65341">Killer heuristic</keyword><keyword weight="0.51095">Search algorithm</keyword><keyword weight="0.66733">Principal variation search</keyword><keyword weight="0.43046">Algorithm</keyword><keyword weight="0.72985">Pruning (decision trees)</keyword><keyword weight="0.63986">Null-move heuristic</keyword><keyword weight="0.50971">Decision tree learning</keyword><keyword weight="0.40374">Mathematics</keyword><keyword weight="0.59261">Pruning</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>We describe an experimental study of pruning methods for decision tree classifiers when the goal is minimizing loss rather than error. In addition to two common methods for error minimization, CARTu0027s cost-complexity pruning and C4.5u0027s error-based pruning, we study the extension of cost-complexity pruning to loss and one pruning variant based on the Laplace correction. We perform an empirical comparison of these methods and evaluate them with respect to loss. We found that applying the Laplace correction to estimate the probability distributions at the leaves was beneficial to all pruning methods. Unlike in error minimization, and somewhat surprisingly, performing no pruning led to results that were on par with other methods in terms of the evaluation criteria. The main advantage of pruning was in the reduction of the decision tree size, sometimes by a factor of ten. While no method dominated others on all datasets, even for the same domain different pruning mechanisms are better for different loss matrices.</abstract></paper>