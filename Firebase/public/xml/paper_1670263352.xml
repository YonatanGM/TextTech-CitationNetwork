<paper id="1670263352"><title>Fast effective rule induction</title><year>1995</year><authors><author org="AT&amp;T Bell Laboratories, 600 Mountain Avenue Murray Hill, NJ 07974" id="2115385359">William W. Cohen</author></authors><n_citation>2928</n_citation><doc_type>Conference</doc_type><references><reference>32120410</reference><reference>146100937</reference><reference>165133269</reference><reference>1510523207</reference><reference>1510806966</reference><reference>1516432974</reference><reference>1531743498</reference><reference>1580451812</reference><reference>1604329830</reference><reference>1638172072</reference><reference>1999138184</reference><reference>2037689320</reference><reference>2039683780</reference><reference>2089967664</reference><reference>2111746072</reference><reference>2128420091</reference></references><venue id="1180662882" type="C">International Conference on Machine Learning</venue><doi>10.1016/B978-1-55860-377-6.50023-2</doi><keywords><keyword weight="0.45026">Computer science</keyword><keyword weight="0.60091">Rule induction</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.46644">Machine learning</keyword></keywords><publisher>Morgan Kaufmann</publisher><abstract>Abstract Many existing rule learning systems are computationally expensive on large noisy datasets. In this paper we evaluate the recently-proposed rule learning algorithm IREP on a large and diverse collection of benchmark problems. We show that while IREP is extremely efficient, it frequently gives error rates higher than those of C4.5 and C4.5rules. We then propose a number of modifications resulting in an algorithm RIPPER k that is very competitive with C4.5rules with respect to error rates, but much more efficient on large samples. RIPPER k obtains error rates lower than or equivalent to C4.5rules on 22 of 37 benchmark problems, scales nearly linearly with the number of training examples, and can efficiently process noisy datasets containing hundreds of thousands of examples.</abstract></paper>