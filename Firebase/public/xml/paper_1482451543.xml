<paper id="1482451543"><title>Multiple decision trees</title><year>1990</year><authors><author org="Basser Department of Computer Science, University of Sydney, Australia#TAB#" id="2282509940">Suk Wah Kwok</author><author org="Basser Department of Computer Science, University of Sydney, Australia#TAB#" id="2791811461">Chris Carter</author></authors><n_citation>81</n_citation><doc_type>Conference</doc_type><references><reference>1985624473</reference><reference>2128420091</reference><reference>2149706766</reference></references><venue id="1204606053" type="C">Uncertainty in Artificial Intelligence</venue><doi>10.1016/B978-0-444-88650-7.50030-5</doi><keywords><keyword weight="0.63291">Tree rearrangement</keyword><keyword weight="0.42581">Computer science</keyword><keyword weight="0.63504">Metric tree</keyword><keyword weight="0.43864">Theoretical computer science</keyword><keyword weight="0.64743">Link/cut tree</keyword><keyword weight="0.70938">Weight-balanced tree</keyword><keyword weight="0.61159">Tree structure</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.63201">Random binary tree</keyword><keyword weight="0.62095">Binary search tree</keyword><keyword weight="0.44061">Machine learning</keyword><keyword weight="0.61266">Decision tree learning</keyword></keywords><publisher>North-Holland Publishing Co.</publisher><abstract>This paper describes experiments, on two domains, to investigate the effect of averaging over predictions of multiple decision trees, instead of using a single tree. Other authors have pointed out theoretical and commonsense reasons for preferring the multiple tree approach. Ideally, we would like to consider predictions from all trees, weighted by their probability. However, there is a vast number of different trees, and it is difficult to estimate the probability of each tree. We sidestep the estimation problem by using a modified version of the ID3 algorithm to build good trees, and average over only these trees. Our results are encouraging. For each domain, we managed to produce a small number of good trees. We find that it is best to average across sets of trees with different structure; this usually gives better performance than any of the constituent trees, including the ID3 tree. Keywords : machine learning, transduction, empirical evaluation</abstract></paper>