<paper id="1575798196"><title>Inducing Probabilistic Grammars by Bayesian Model Merging</title><year>1994</year><authors><author org="International Computer Science Institute" id="684032073">Andreas Stolcke</author><author org="International Computer Science Institute" id="2797595859">Stephen M. Omohundro</author></authors><n_citation>191</n_citation><doc_type>Conference</doc_type><references><reference>1486103693</reference><reference>1583239513</reference><reference>1825362480</reference><reference>1983661866</reference><reference>2002582188</reference><reference>2056933273</reference><reference>2061079066</reference><reference>2096498841</reference><reference>2111041233</reference><reference>2121227244</reference><reference>2157140289</reference><reference>2162157640</reference></references><venue id="1130809216" type="C">International Colloquium on Grammatical Inference</venue><doi>10.1007/3-540-58473-0_141</doi><keywords><keyword weight="0.67536">Stochastic context-free grammar</keyword><keyword weight="0.57895">Link grammar</keyword><keyword weight="0.66437">L-attributed grammar</keyword><keyword weight="0.52862">Bayesian inference</keyword><keyword weight="0.64872">Context-free grammar</keyword><keyword weight="0.43231">Computer science</keyword><keyword weight="0.51827">Posterior probability</keyword><keyword weight="0.44624">Natural language processing</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.51518">Hidden Markov model</keyword><keyword weight="0.6355">Stochastic grammar</keyword><keyword weight="0.4425">Machine learning</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (‘Occamu0027s Razor’). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based n-grams, and stochastic context-free grammars.</abstract></paper>