<paper id="1523064285"><title>Comparing OpenBRR, QSOS, and OMM Assessment Models</title><year>2010</year><authors><author org="Free University of Bolzano" id="303037059">Etiel Petrinja</author><author org="Free University of Bolzano" id="120576184">Alberto Sillitti</author><author org="Free University of Bolzano" id="2310608531">Giancarlo Succi</author></authors><n_citation>27</n_citation><doc_type>Conference</doc_type><references><reference>1552836223</reference><reference>1590479000</reference><reference>2087053207</reference><reference>2111199933</reference><reference>2163851162</reference></references><venue id="1176326080" type="C">Open Source Systems</venue><doi>10.1007/978-3-642-13244-5_18</doi><keywords><keyword weight="0.51912">Test plan</keyword><keyword weight="0.43021">Systems engineering</keyword><keyword weight="0.44573">Software engineering</keyword><keyword weight="0.42764">Computer science</keyword><keyword weight="0.56849">Usability</keyword><keyword weight="0.51898">Capability Maturity Model</keyword><keyword weight="0.55822">QSOS</keyword><keyword weight="0.61991">Software quality</keyword><keyword weight="0.0">Open source software</keyword><keyword weight="0.43542">Database</keyword></keywords><publisher>Springer Berlin Heidelberg</publisher><abstract>The objective of this study was to investigate the quality and usability of three Free/Libre Open Source Software assessment models: the Open Business Readiness Rating (OpenBRR), the Qualification and Selection of Open Source software (QSOS), and the QualiPSo OpenSource Maturity Model (OMM). The study identified the positive and negative aspects of each of them. The models were used to assess two Free/Libre Open Source Software projects: Firefox and Chrome (Chromium). The study is based on a set of controlled experiments in which the participants performed the assessment using only one model each. The model used and the Free/Libre Open Source Software project assessed were randomly assigned to the participants. The experiment was conducted in a controlled environment with defined tasks to be performed in a given time interval. The results revealed that the three models provided comparable assessments for the two assessed projects. The main conclusion was that all the three models contain some questions and proposed answers that are not clear to the assessors, therefore should be rewritten or explained better. The critical aspects of each model were: Functionality and Quality for OpenBRR; Adoption, Administration/Monitoring, Copyright owners, and Browser for QSOS ; and Quality of the Test Plan, and the Technical Environment for OMM. Participants perceived the quality and usability of the three models of comparable level.</abstract></paper>