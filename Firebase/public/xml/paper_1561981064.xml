<paper id="1561981064"><title>Simulation Approaches to General Probabilistic Inference on Belief Networks</title><year>1990</year><authors><author org="Department of Engineering-Economic Systems, Stanford University, Terman Engineering Center, Stanford, CA 94305-4025" id="53644460">Ross D. Shachter</author><author org="Department of Engineering-Economic Systems, Stanford University, and Rockwell International Science Center, Palo Alto Laboratory, 444 High Street, Suite 400, Palo Alto, CA 94301" id="662435111">Mark A. Peot</author></authors><n_citation>201</n_citation><doc_type>Conference</doc_type><references><reference>20626266</reference><reference>1968323507</reference><reference>1980452149</reference><reference>2093976733</reference><reference>2113677269</reference><reference>2114307208</reference><reference>2143075689</reference><reference>2173497437</reference></references><venue id="1204606053" type="C">Uncertainty in Artificial Intelligence</venue><doi>10.1016/B978-0-444-88738-2.50024-5</doi><keywords><keyword weight="0.0">Probabilistic inference</keyword><keyword weight="0.51037">Monte Carlo method</keyword><keyword weight="0.52398">Conditional probability</keyword><keyword weight="0.44953">Computer science</keyword><keyword weight="0.55339">Conditional independence</keyword><keyword weight="0.54219">Markov chain</keyword><keyword weight="0.46641">Theoretical computer science</keyword><keyword weight="0.43746">Exploit</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.52261">Sampling (statistics)</keyword><keyword weight="0.60104">Probabilistic logic</keyword><keyword weight="0.46085">Machine learning</keyword></keywords><publisher>North-Holland Publishing Co.</publisher><abstract>A number of algorithms have been developed to solve probabilistic inference problems on belief networks. These algorithms can be divided into two main groups: exact techniques which exploit the conditional independence revealed when the graph structure is relatively sparse, and probabilistic sampling techniques which exploit the “conductance” of an embedded Markov chain when the conditional probabilities have non-extreme values. In this paper, we investigate a family of “forward” Monte Carlo sampling techniques similar to Logic Sampling [Henrion, 1988] which appear to perform well even in some multiplyconnected networks with extreme conditional probabilities, and thus would be generally applicable. We consider several enhancements which reduce the posterior variance using this approach and propose a framework and criteria for choosing when to use those enhancements.</abstract></paper>