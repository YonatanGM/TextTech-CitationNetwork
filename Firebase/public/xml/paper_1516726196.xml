<paper id="1516726196"><title>An algorithmic theory of learning: robust concepts and random projection</title><year>1999</year><authors><author org="Dept. of Psychol., Harvard Univ., Boston, MA, USA" id="1580353647">R.I. Arriaga</author><author org="Department of Mathematics, M.I.T., cambridge," id="2045140648">S. Vempala</author></authors><n_citation>275</n_citation><doc_type>Conference</doc_type><references><reference>1519844777</reference><reference>1520252399</reference><reference>1938180907</reference><reference>1971238646</reference><reference>1975846642</reference><reference>1979711143</reference><reference>1991095526</reference><reference>2006863141</reference><reference>2019363670</reference><reference>2041268952</reference><reference>2046242327</reference><reference>2062083887</reference><reference>2086789740</reference><reference>2119821739</reference><reference>2122308921</reference><reference>2129113961</reference><reference>2129192653</reference><reference>2141793995</reference><reference>2143362693</reference><reference>2147717514</reference><reference>2154952480</reference><reference>2156909104</reference><reference>2161959472</reference><reference>2171574551</reference></references><venue id="1150208541" type="C">Foundations of Computer Science</venue><doi>10.1007/s10994-006-6265-7</doi><keywords><keyword weight="0.50422">Random projection</keyword><keyword weight="0.51936">Boolean function</keyword><keyword weight="0.54863">Algorithmics</keyword><keyword weight="0.46099">Computer science</keyword><keyword weight="0.51565">Support vector machine</keyword><keyword weight="0.54668">Concept learning</keyword><keyword weight="0.52676">Robustness (computer science)</keyword><keyword weight="0.46646">Artificial intelligence</keyword><keyword weight="0.51124">Time complexity</keyword><keyword weight="0.46191">Machine learning</keyword><keyword weight="0.50561">Randomness</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>We study the phenomenon of cognitive learning from an algorithmic standpoint. How does the brain effectively learn concepts from a small number of examples despite the fact that each example contains a huge amount of information? We provide a novel analysis for a model of robust concept learning (closely related to "margin classifiers"), and show that a relatively small number of examples are sufficient to learn rich concept classes (including threshold functions, Boolean formulae and polynomial surfaces). As a result, we obtain simple intuitive proofs for the generalization bounds of Support Vector Machines. In addition, the new algorithm has several advantages-they are faster conceptually simpler and highly resistant to noise. For example, a robust half-space can be PAC-learned in linear time using only a constant number of training examples, regardless of the number of attributes. A general (algorithmic) consequence of the model, that "more robust concepts are easier to learn", is supported by a multitude of psychological studies.</abstract></paper>