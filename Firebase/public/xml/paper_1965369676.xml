<paper id="1965369676"><title>Improving backpropagation learning with feature selection</title><year>1996</year><authors><author org="Department of Information Systems &amp; Computer Science, National University of Singapore, Singapore, Republic of Singapore" id="2028249872">Rudy Setiono</author><author org="Department of Information Systems &amp; Computer Science, National University of Singapore, Singapore, Republic of Singapore" id="2122391114">Huan Liu</author></authors><n_citation>34</n_citation><doc_type>Journal</doc_type><references><reference>196808700</reference><reference>1579665324</reference><reference>2078409719</reference><reference>2087836750</reference><reference>2093190817</reference><reference>2099791377</reference><reference>2109779438</reference><reference>2128033389</reference><reference>2149706766</reference></references><venue id="74726891" type="J">Applied Intelligence</venue><doi>10.1007/BF00117813</doi><keywords><keyword weight="0.47621">Data mining</keyword><keyword weight="0.0">Noisy data</keyword><keyword weight="0.55475">Feature selection</keyword><keyword weight="0.45907">Computer science</keyword><keyword weight="0.42313">If and only if</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.47242">Speedup</keyword><keyword weight="0.50356">Information theory</keyword><keyword weight="0.0">Network on</keyword><keyword weight="0.60208">Feedforward neural network</keyword><keyword weight="0.4758">Pattern recognition</keyword><keyword weight="0.55955">Backpropagation</keyword><keyword weight="0.47774">Machine learning</keyword></keywords><publisher>Springer Netherlands</publisher><abstract>There exist redundant, irrelevant and noisy data. Using proper data to train a network can speed up training, simplify the learned structure, and improve its performance. A two-phase training algorithm is proposed. In the first phase, the number of input units of the network is determined by using an information base method. Only those attributes that meet certain criteria for inclusion will be considered as the input to the network. In the second phase, the number of hidden units of the network is selected automatically based on the performance of the network on the training data. One hidden unit is added at a time only if it is necessary. The experimental results show that this new algorithm can achieve a faster learning time, a simpler network and an improved performance.</abstract></paper>