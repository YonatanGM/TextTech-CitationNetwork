<paper id="1821109219"><title>Robust computation and parametrization of multiple view relations</title><year>1998</year><authors><author org="Dept. of Eng. Sci. Oxford Univ., UK" id="2107932047">P. Torr</author><author org="" id="2469405535">A. Zisserman</author></authors><n_citation>146</n_citation><doc_type>Conference</doc_type><references><reference>1482517735</reference><reference>1848858524</reference><reference>1949154667</reference><reference>2027218073</reference><reference>2085261163</reference><reference>2134211982</reference><reference>2170026011</reference></references><venue id="1164975091" type="C">International Conference on Computer Vision</venue><doi>10.1109/ICCV.1998.710798</doi><keywords><keyword weight="0.44308">Computer vision</keyword><keyword weight="0.49402">Parametrization</keyword><keyword weight="0.42546">Computer science</keyword><keyword weight="0.47073">Matrix (mathematics)</keyword><keyword weight="0.48402">Quadratic equation</keyword><keyword weight="0.49463">Robustness (computer science)</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.53808">Real image</keyword><keyword weight="0.50267">Focus (optics)</keyword><keyword weight="0.50127">Constrained optimization</keyword><keyword weight="0.47151">Computation</keyword></keywords><publisher>IEEE</publisher><abstract>A new method is presented for robustly estimating multiple view relations from image point correspondences. There are three new contributions, the first is a general purpose method of parametrizing these relations using point correspondences. The second contribution is the formulation of a common Maximum Likelihood Estimate (MLE) for each of the multiple view relations. The parametrization facilitates a constrained optimization to obtain this MLE. The third contribution is a new robust algorithm, MLESAC, for obtaining the point correspondences. The method is general and its use is illustrated for the estimation of fundamental matrices, image to image homographies and quadratic transformations. Results are given for both synthetic and real images. It is demonstrated that the method gives results equal or superior to previous approaches.</abstract></paper>