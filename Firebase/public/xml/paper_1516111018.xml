<paper id="1516111018"><title>An introduction to variational methods for graphical models</title><year>1999</year><authors><author org="Department of Electrical Engineering and Computer Sciences and Department of Statistics, University of California, Berkeley, CA 94720, USA. jordan@cs.berkeley.edu#TAB#" id="2435751034">Michael I. Jordan</author><author org="Gatsby Computational Neuroscience Unit, University College London WC1N 3AR, UK. zoubin@gatsby.ucl.ac.uk#TAB#" id="617670330">Zoubin Ghahramani</author><author org="Artificial Intelligence Laboratory, MIT, Cambridge, MA 02139, USA. tommi@ai.mit.edu#TAB#" id="340253981">Tommi S. Jaakkola</author><author org="AT&amp;T Labsâ€“Research, Florham Park, NJ 07932, USA. lsaul@research.att.edu#TAB#" id="2143047568">Lawrence K. Saul</author></authors><n_citation>2550</n_citation><doc_type>Journal</doc_type><references><reference>1510302714</reference><reference>1520448186</reference><reference>1528056001</reference><reference>1578685895</reference><reference>1587990862</reference><reference>1592590442</reference><reference>1790295154</reference><reference>1829360078</reference><reference>1919989817</reference><reference>1972748205</reference><reference>1999321705</reference><reference>1999432334</reference><reference>2026799324</reference><reference>2047229728</reference><reference>2075450508</reference><reference>2083380015</reference><reference>2090361527</reference><reference>2104163628</reference><reference>2105658140</reference><reference>2109787716</reference><reference>2112798704</reference><reference>2116723448</reference><reference>2128023014</reference><reference>2129031807</reference><reference>2133231442</reference><reference>2145211911</reference><reference>2145550984</reference><reference>2147496287</reference><reference>2159080219</reference><reference>2167794538</reference><reference>2175714895</reference><reference>2725061391</reference></references><venue id="62148650" type="J">Machine Learning</venue><doi>10.1023/A:1007665907178</doi><keywords><keyword weight="0.55191">Boltzmann machine</keyword><keyword weight="0.56479">Inference</keyword><keyword weight="0.55017">Markov chain</keyword><keyword weight="0.61426">Approximate inference</keyword><keyword weight="0.58497">Bayesian network</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.6523">Variational Bayesian methods</keyword><keyword weight="0.66444">Graphical model</keyword><keyword weight="0.45321">Machine learning</keyword><keyword weight="0.409">Mathematics</keyword><keyword weight="0.63743">Variational message passing</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.</abstract></paper>