<paper id="1709057194"><title>The Relaxed Online Maximum Margin Algorithm</title><year>1999</year><authors><author org="Department of Engineering Mathematics, Queen's Building, University of Bristol, Bristol BS8 1TR, UK. Y.Li@bristol.ac.uk#TAB#" id="2607805406">Yi Li</author><author org="Department of Computer Science, National University of Singapore, Singapore 117543, Republic of Singapore. plong@comp.nus.edu.sg#TAB#" id="2135968533">Philip M. Long</author></authors><n_citation>194</n_citation><doc_type>Conference</doc_type><references><reference>1496612019</reference><reference>1537299585</reference><reference>1975846642</reference><reference>1976026757</reference><reference>1979711143</reference><reference>2017031328</reference><reference>2040293685</reference><reference>2087347434</reference><reference>2091401625</reference><reference>2094062207</reference><reference>2106491486</reference><reference>2108136473</reference><reference>2119821739</reference><reference>2129056047</reference><reference>2129113961</reference><reference>2133671888</reference><reference>2139999468</reference><reference>2156909104</reference><reference>2157239837</reference><reference>2158078575</reference><reference>2166282318</reference><reference>2168420538</reference></references><venue id="1127325140" type="C">Neural Information Processing Systems</venue><doi>10.1023/A:1012435301888</doi><keywords><keyword weight="0.55216">Margin (machine learning)</keyword><keyword weight="0.42829">Computer science</keyword><keyword weight="0.6089">FSA-Red Algorithm</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.62414">Population-based incremental learning</keyword><keyword weight="0.61303">Weighted Majority Algorithm</keyword><keyword weight="0.45662">Mathematical optimization</keyword><keyword weight="0.63803">Ramer–Douglas–Peucker algorithm</keyword><keyword weight="0.46795">Algorithm</keyword><keyword weight="0.61402">Winnow</keyword><keyword weight="0.56774">Margin classifier</keyword><keyword weight="0.56422">Perceptron</keyword><keyword weight="0.43571">Machine learning</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>We describe a new incremental algorithm for training linear threshold functions: the Relaxed Online Maximum Margin Algorithm, or ROMMA. ROMMA can be viewed as an approximation to the algorithm that repeatedly chooses the hyperplane that classifies previously seen examples correctly with the maximum margin. It is known that such a maximum-margin hypothesis can be computed by minimizing the length of the weight vector subject to a number of linear constraints. ROMMA works by maintaining a relatively simple relaxation of these constraints that can be efficiently updated. We prove a mistake bound for ROMMA that is the same as that proved for the perceptron algorithm. Our analysis implies that the maximum-margin algorithm also satisfies this mistake bounds this is the first worst-case performance guarantee for this algorithm. We describe some experiments using ROMMA and a variant that updates its hypothesis more aggressively as batch algorithms to recognize handwritten digits. The computational complexity and simplicity of these algorithms is similar to that of perceptron algorithm, but their generalization is much better. We show that a batch algorithm based on aggressive ROMMA converges to the fixed threshold SVM hypothesis.</abstract></paper>