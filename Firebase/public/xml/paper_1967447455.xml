<paper id="1967447455"><title>Interactive Video Indexing With Statistical Active Learning</title><year>2012</year><authors><author org="Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore" id="2249802926">Zheng-Jun Zha</author><author org="Hefei University of Technology, Hefei, China;" id="2915826832">Meng Wang</author><author org="Inst. for Infocomm Res., Singapore, Singapore" id="2308800322">Yan-Tao Zheng</author><author org="Sch. of Inf. Technol. &amp; Electr. Eng., Univ. of Queensland, Brisbane, QLD, Australia" id="2430169722">Yi Yang</author><author org="Hefei University of Technology, Hefei, China;" id="2129930771">Richang Hong</author><author org="Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore" id="2160663097">Tat-Seng Chua</author></authors><n_citation>122</n_citation><doc_type>Journal</doc_type><references><reference>22751559</reference><reference>1480376833</reference><reference>1484084878</reference><reference>1485810016</reference><reference>1533011499</reference><reference>1549658602</reference><reference>1845402413</reference><reference>1860923984</reference><reference>1978633512</reference><reference>1981745143</reference><reference>1988732650</reference><reference>2012599677</reference><reference>2026327172</reference><reference>2038967169</reference><reference>2062879799</reference><reference>2070076664</reference><reference>2094679190</reference><reference>2095609079</reference><reference>2098539983</reference><reference>2100801279</reference><reference>2104290444</reference><reference>2112301781</reference><reference>2115115103</reference><reference>2118168768</reference><reference>2118236796</reference><reference>2130941826</reference><reference>2132292587</reference><reference>2137117795</reference><reference>2138079527</reference><reference>2142126424</reference><reference>2143854982</reference><reference>2148809503</reference><reference>2155906060</reference><reference>2156421615</reference><reference>2165484066</reference><reference>2166083072</reference><reference>2168728412</reference></references><venue id="137030581" type="J">IEEE Transactions on Multimedia</venue><doi>10.1109/TMM.2011.2174782</doi><keywords><keyword weight="0.43262">Least squares</keyword><keyword weight="0.47672">Data mining</keyword><keyword weight="0.45669">Computer science</keyword><keyword weight="0.51956">Search engine indexing</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.51898">Interactive video</keyword><keyword weight="0.45652">Computer vision</keyword><keyword weight="0.47644">Algorithm design</keyword><keyword weight="0.55941">Active learning</keyword><keyword weight="0.46792">Pattern recognition</keyword><keyword weight="0.5596">TRECVID</keyword><keyword weight="0.43531">Exploit</keyword><keyword weight="0.47788">Machine learning</keyword><keyword weight="0.44391">Semantics</keyword></keywords><publisher>IEEE</publisher><abstract>Video indexing, also called video concept detection, has attracted increasing attentions from both academia and industry. To reduce human labeling cost, active learning has been introduced to video indexing recently. In this paper, we propose a novel active learning approach based on the optimum experimental design criteria in statistics. Different from existing optimum experimental design, our approach simultaneously exploits sampleu0027s local structure, and sample relevance, density, and diversity information, as well as makes use of labeled and unlabeled data. Specifically, we develop a local learning model to exploit the local structure of each sample. Our assumption is that for each sample, its label can be well estimated based on its neighbors. By globally aligning the local models from all the samples, we obtain a local learning regularizer, based on which a local learning regularized least square model is proposed. Finally, a unified sample selection approach is developed for interactive video indexing, which takes into account the sample relevance, density and diversity information, and sample efficacy in minimizing the parameter variance of the proposed local learning regularized least square model. We compare the performance between our approach and the state-of-the-art approaches on the TREC video retrieval evaluation (TRECVID) benchmark. We report superior performance from the proposed approach.</abstract></paper>