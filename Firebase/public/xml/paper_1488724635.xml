<paper id="1488724635"><title>Computing Optimal Hypotheses Efficiently for Boosting</title><year>2002</year><authors><author org="University of Tokyo" id="1972520240">Shinichi Morishita</author></authors><n_citation>17</n_citation><doc_type>Conference</doc_type><references><reference>61958748</reference><reference>168014760</reference><reference>1506285740</reference><reference>1531508514</reference><reference>1988790447</reference><reference>2029817244</reference><reference>2046995465</reference><reference>2060164640</reference><reference>2070534370</reference><reference>2071091006</reference><reference>2103285706</reference><reference>2112076978</reference><reference>2136003390</reference><reference>2152761983</reference><reference>2166559705</reference><reference>2336485197</reference></references><venue id="1132231445" type="C">Discovery Science</venue><doi>10.1007/3-540-45884-0_35</doi><keywords><keyword weight="0.46019">Data mining</keyword><keyword weight="0.50232">Decision tree</keyword><keyword weight="0.43233">Data processing</keyword><keyword weight="0.61616">AdaBoost</keyword><keyword weight="0.49868">Categorical variable</keyword><keyword weight="0.44941">Computer science</keyword><keyword weight="0.46081">Information extraction</keyword><keyword weight="0.4617">Artificial intelligence</keyword><keyword weight="0.57969">Boosting (machine learning)</keyword><keyword weight="0.54554">Overfitting</keyword><keyword weight="0.48645">Majority rule</keyword><keyword weight="0.47003">Machine learning</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>This paper sheds light on a strong connection between AdaBoost and several optimization algorithms for data mining. AdaBoost has been the subject of much interests as an effective methodology for classification task. AdaBoost repeatedly generates one hypothesis in each round, and finally it is able to make a highly accurate prediction by taking a weighted majority vote on the resulting hypotheses. Freund and Schapire have remarked that the use of simple hypotheses such as single-test decision trees instead of huge trees would be promising for achieving high accuracy and avoiding overfitting to the training data. One major drawback of this approach however is that accuracies of simple individual hypotheses may not always be high, hence demanding a way of computing more accurate (or, the most accurate) simple hypotheses efficiently. In this paper, we consider several classes of simple but expressive hypotheses such as ranges and regions for numeric attributes, subsets of categorical values, and conjunctions of Boolean tests. For each class, we develop an efficient algorithm for choosing the optimal hypothesis.</abstract></paper>