<paper id="1512558044"><title>The Leave-One-Out Kernel</title><year>2002</year><authors><author org="AIST CBRC" id="1858955830">Koji Tsuda</author><author org="Fraunhofer FIRST" id="2072849696">Motoaki Kawanabe</author></authors><n_citation>6</n_citation><doc_type>Conference</doc_type><references><reference>1971784203</reference><reference>2108995755</reference><reference>2155148037</reference><reference>2166473218</reference><reference>2283504545</reference></references><venue id="1158833223" type="C">International Conference on Artificial Neural Networks</venue><doi>10.1007/3-540-46084-5_118</doi><keywords><keyword weight="0.44735">Applied mathematics</keyword><keyword weight="0.43244">Discrete mathematics</keyword><keyword weight="0.42562">Mathematical optimization</keyword><keyword weight="0.69666">Radial basis function kernel</keyword><keyword weight="0.72128">Kernel embedding of distributions</keyword><keyword weight="0.68882">Kernel Fisher discriminant analysis</keyword><keyword weight="0.71907">Kernel principal component analysis</keyword><keyword weight="0.70038">Polynomial kernel</keyword><keyword weight="0.75853">Variable kernel density estimation</keyword><keyword weight="0.69498">Kernel regression</keyword><keyword weight="0.41938">Mathematics</keyword><keyword weight="0.71645">Kernel (statistics)</keyword></keywords><publisher>Springer Berlin Heidelberg</publisher><abstract>Recently, several attempts have been made for deriving datadependent kernels from distribution estimates withparametric models (e.g. the Fisher kernel). In this paper, we propose a new kernel derived from any distribution estimators, parametric or nonparametric. This kernel is called the Leave-one-out kernel (i.e. LOO kernel), because the leave-one-out process plays an important role to compute this kernel. We will show that, when applied to a parametric model, the LOO kernel converges to the Fisher kernel asymptotically as the number of samples goes to infinity.</abstract></paper>