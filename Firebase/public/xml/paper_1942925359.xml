<paper id="1942925359"><title>Active Sampling for Class Probability Estimation and Ranking</title><year>2004</year><authors><author org="Department of Management Science and Information Systems, Red McCombs School of Business, The University of Texas at Austin, Austin, Texas 78712, USA. maytal.saar-tsechansky@bus.utexas.edu ...#TAB#" id="131186531">Maytal Saar-Tsechansky</author><author org="Department of Information Operations &amp; Management Sciences, Leonard N. Stern School of Business, New York University, 44 West Fourth Street, New York, NY 10012, USA. fprovost@stern.nyu ...#TAB#" id="2158932634">Foster Provost</author></authors><n_citation>178</n_citation><doc_type>Journal</doc_type><references><reference>203696055</reference><reference>1484084878</reference><reference>1513874326</reference><reference>1514707997</reference><reference>1524761913</reference><reference>1554734378</reference><reference>1819386543</reference><reference>1840338487</reference><reference>1970185999</reference><reference>1977245551</reference><reference>2018770010</reference><reference>2018810220</reference><reference>2019363670</reference><reference>2061416491</reference><reference>2080021732</reference><reference>2085989833</reference><reference>2089933214</reference><reference>2112076978</reference><reference>2125055259</reference><reference>2135892731</reference><reference>2139709458</reference><reference>2151023586</reference><reference>2152761983</reference><reference>2155653793</reference><reference>2163476003</reference><reference>2912934387</reference></references><venue id="62148650" type="J">Machine Learning</venue><doi>10.1023/B:MACH.0000011806.12374.c3</doi><keywords><keyword weight="0.53569">Decision tree</keyword><keyword weight="0.55776">Active learning</keyword><keyword weight="0.61272">Semi-supervised learning</keyword><keyword weight="0.595">Active learning (machine learning)</keyword><keyword weight="0.45552">Pattern recognition</keyword><keyword weight="0.50954">Ranking</keyword><keyword weight="0.44919">Computer science</keyword><keyword weight="0.5869">Supervised learning</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.61998">Sampling (statistics)</keyword><keyword weight="0.47229">Machine learning</keyword><keyword weight="0.54771">Gibbs sampling</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>In many cost-sensitive environments class probability estimates are used by decision makers to evaluate the expected utility from a set of alternatives. Supervised learning can be used to build class probability estimates; however, it often is very costly to obtain training data with class labels. Active learning acquires data incrementally, at each phase identifying especially useful additional data for labeling, and can be used to economize on examples needed for learning. We outline the critical features of an active learner and present a sampling-based active learning method for estimating class probabilities and class-based rankings. BOOTSTRAP-LV identifies particularly informative new data for learning based on the variance in probability estimates, and uses weighted sampling to account for a potential exampleu0027s informative value for the rest of the input space. We show empirically that the method reduces the number of data items that must be obtained and labeled, across a wide variety of domains. We investigate the contribution of the components of the algorithm and show that each provides valuable information to help identify informative examples. We also compare BOOTSTRAP-LV with UNCERTAINTY SAMPLING, an existing active learning method designed to maximize classification accuracy. The results show that BOOTSTRAP-LV uses fewer examples to exhibit a certain estimation accuracy and provide insights to the behavior of the algorithms. Finally, we experiment with another new active sampling algorithm drawing from both UNCERTAINTY SAMPLING and BOOTSTRAP-LV and show that it is significantly more competitive with BOOTSTRAP-LV compared to UNCERTAINTY SAMPLING. The analysis suggests more general implications for improving existing active sampling algorithms for classification.</abstract></paper>