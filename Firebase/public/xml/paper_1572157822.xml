<paper id="1572157822"><title>Forgetting Exceptions is Harmful in Language Learning</title><year>1999</year><authors><author org="ILK / Computational Linguistics, Tilburg University, P.O. Box 90153, NL-5000 LE Tilburg, The Netherlands. [email protected]#TAB#" id="2234963844">Walter Daelemans</author><author org="ILK / Computational Linguistics, Tilburg University, P.O. Box 90153, NL-5000 LE Tilburg, The Netherlands. [email protected]#TAB#" id="2161053677">Antal Van Den Bosch</author><author org="ILK / Computational Linguistics, Tilburg University, P.O. Box 90153, NL-5000 LE Tilburg, The Netherlands. [email protected]#TAB#" id="281738054">Jakub Zavrel</author></authors><n_citation>232</n_citation><doc_type>Journal</doc_type><references><reference>7389446</reference><reference>21067618</reference><reference>146100937</reference><reference>207612057</reference><reference>1483055888</reference><reference>1512628935</reference><reference>1517267840</reference><reference>1517749756</reference><reference>1539739406</reference><reference>1551773846</reference><reference>1580142630</reference><reference>1586887050</reference><reference>1592282508</reference><reference>1601728146</reference><reference>1604115992</reference><reference>1623072288</reference><reference>1632114991</reference><reference>1689445748</reference><reference>1795234945</reference><reference>1924403233</reference><reference>1982499516</reference><reference>1993989674</reference><reference>2001587401</reference><reference>2003048487</reference><reference>2004131797</reference><reference>2015042937</reference><reference>2023696990</reference><reference>2092186523</reference><reference>2096123080</reference><reference>2099247782</reference><reference>2102997946</reference><reference>2107686700</reference><reference>2122111042</reference><reference>2125055259</reference><reference>2134237567</reference><reference>2134980541</reference><reference>2135479218</reference><reference>2142334564</reference><reference>2145269499</reference><reference>2147169507</reference><reference>2149706766</reference><reference>2155063582</reference><reference>2157025692</reference><reference>2167277498</reference><reference>2167299284</reference><reference>2605974740</reference><reference>2963847008</reference></references><venue id="62148650" type="J">Machine Learning</venue><doi>10.1023/A:1007585615670</doi><keywords><keyword weight="0.60755">Algorithmic learning theory</keyword><keyword weight="0.60341">Semi-supervised learning</keyword><keyword weight="0.64112">Instance-based learning</keyword><keyword weight="0.57941">Active learning (machine learning)</keyword><keyword weight="0.43981">Computer science</keyword><keyword weight="0.45796">Natural language processing</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.59873">Sequence learning</keyword><keyword weight="0.5811">Learning classifier system</keyword><keyword weight="0.60668">Multi-task learning</keyword><keyword weight="0.60009">Stability (learning theory)</keyword><keyword weight="0.42737">Pattern recognition</keyword><keyword weight="0.45251">Machine learning</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>We show that in language learning, contrary to received wisdom, keeping exceptional training instances in memory can be beneficial for generalization accuracy. We investigate this phenomenon empirically on a selection of benchmark natural language processing tasks: grapheme-to-phoneme conversion, part-of-speech tagging, prepositional-phrase attachment, and base noun phrase chunking. In a first series of experiments we combine memory-based learning with training set editing techniques, in which instances are edited based on their typicality and class prediction strength. Results show that editing exceptional instances (with low typicality or low class prediction strength) tends to harm generalization accuracy. In a second series of experiments we compare memory-based learning and decision-tree learning methods on the same selection of tasks, and find that decision-tree learning often performs worse than memory-based learning. Moreover, the decrease in performance can be linked to the degree of abstraction from exceptions (i.e., pruning or eagerness). We provide explanations for both results in terms of the properties of the natural language processing tasks and the learning algorithms.</abstract></paper>