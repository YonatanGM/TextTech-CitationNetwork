<paper id="1967370444"><title>Sound and complete relevance assessment for XML retrieval</title><year>2008</year><authors><author org="Yahoo&amp;excl; Research Latin America" id="210095896">Benjamin Piwowarski</author><author org="Univ. of Otago, Dunedin, New Zealand" id="2110053611">Andrew Trotman</author><author org="Queen Mary, University of London, U.K" id="46148421">Mounia Lalmas</author></authors><n_citation>200</n_citation><doc_type>Journal</doc_type><references><reference>15201044</reference><reference>36405575</reference><reference>115453883</reference><reference>1482549127</reference><reference>1529317928</reference><reference>1530834279</reference><reference>2023515034</reference><reference>2035820422</reference><reference>2041234563</reference><reference>2045117606</reference><reference>2074700442</reference><reference>2075893676</reference><reference>2085215424</reference><reference>2090111998</reference><reference>2120308175</reference><reference>2129031474</reference><reference>2156296249</reference><reference>2156879337</reference><reference>2163511272</reference><reference>2505977556</reference><reference>2916949817</reference></references><venue id="87067389" type="J">ACM Transactions on Information Systems</venue><doi>10.1145/1416950.1416951</doi><keywords><keyword weight="0.41891">Cyberchondria</keyword><keyword weight="0.44164">Data mining</keyword><keyword weight="0.67509">Human–computer information retrieval</keyword><keyword weight="0.47503">Information retrieval</keyword><keyword weight="0.56871">XML</keyword><keyword weight="0.43503">Computer science</keyword><keyword weight="0.63943">Data retrieval</keyword><keyword weight="0.0">XML retrieval</keyword><keyword weight="0.69328">Relevance (information retrieval)</keyword><keyword weight="0.62079">Document retrieval</keyword></keywords><publisher>ACM</publisher><abstract>In information retrieval research, comparing retrieval approaches requires test collections consisting of documents, user requests and relevance assessments. Obtaining relevance assessments that are as sound and complete as possible is crucial for the comparison of retrieval approaches. In XML retrieval, the problem of obtaining sound and complete relevance assessments is further complicated by the structural relationships between retrieval results. A major difference between XML retrieval and flat document retrieval is that the relevance of elements (the retrievable units) is not independent of that of related elements. This has major consequences for the gathering of relevance assessments. This article describes investigations into the creation of sound and complete relevance assessments for the evaluation of content-oriented XML retrieval as carried out at INEX, the evaluation campaign for XML retrieval. The campaign, now in its seventh year, has had three substantially different approaches to gather assessments and has finally settled on a highlighting method for marking relevant passages within documents—even though the objective is to collect assessments at element level. The different methods of gathering assessments at INEX are discussed and contrasted. The highlighting method is shown to be the most reliable of the methods.</abstract></paper>