<paper id="1584560347"><title>Foundations of Secure Interactive Computing</title><year>1991</year><authors><author org="Penn-State Univ" id="2124380882">Donald Beaver</author></authors><n_citation>227</n_citation><doc_type>Conference</doc_type><references><reference>590722630</reference><reference>1540442590</reference><reference>1603973540</reference><reference>1605900190</reference><reference>1970606468</reference><reference>1984976620</reference><reference>2019573778</reference><reference>2023906228</reference><reference>2033766329</reference><reference>2060928064</reference><reference>2107324709</reference><reference>2142222275</reference><reference>2151433956</reference><reference>2161670619</reference><reference>2405007439</reference></references><venue id="1153524033" type="C">International Cryptology Conference</venue><doi>10.1007/3-540-46766-1_31</doi><keywords><keyword weight="0.47088">Psychological resilience</keyword><keyword weight="0.54281">Secure multi-party computation</keyword><keyword weight="0.46787">Computer science</keyword><keyword weight="0.53178">Correctness</keyword><keyword weight="0.46683">Theoretical computer science</keyword><keyword weight="0.47856">Interactive computing</keyword><keyword weight="0.53144">Mathematical proof</keyword><keyword weight="0.47723">Turing</keyword><keyword weight="0.48392">Folk theorem</keyword><keyword weight="0.56215">Oblivious transfer</keyword><keyword weight="0.45322">Distributed computing</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>The problem of secure multiparty computation is usually described as follows: each of n players in a network holds a private input xi. Together they would like to compute a function F(x1,...,xn) without revealing the inputs, even though no particular player can be trusted. Attempts to contrive formal definitions for the problem have treated properties of the solution separately (correctness, privacy, etc.), giving an ad hoc collection of desirable properties and varied definitions that do not support clear or comparable proofs.We propose a clear, concise, and unified definition for security and reliability in interactive computations. We develop a reduction called relative resilience that captures all desired properties at a single blow. Relative resilience allows one to classify and compare arbitrary protocols in terms of security and reliability, in the same way that Turing reductions allow one to classify and compare algorithms in terms of complexity. Security and reliability reduce to a simple statement: a protocol for F is resilient if it is as resilient as an ideal protocol in which a trusted host is available to compute F. Relative resilience captures the notions of security and reliability for a wide variety of interactive computations, including zero-knowledge proof systems, Byzantine Agreement, oblivious transfer, two-party oblivious circuit evaluation, among others.Relative resilience provides modular proof techniques that other approaches lack: one may compare a sequence of protocols ranging from the real-world protocol to the ideal protocol, proving the relative resilience of each successive protocol with greater clarity and less complexity. Folk theorems about the "transitivity" of security and the security of concatenated protocols are now provable; and the proofs reveal that such folk theorems fail under subtle conditions that have previously gone unnoticed. The conciseness and modularity of our definitions and proof techniques provide great clarity in designing and reasoning about protocols and have already lead to provably secure protocols that are significantly more efficient than those appearing in the literature.</abstract></paper>