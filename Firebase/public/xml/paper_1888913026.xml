<paper id="1888913026"><title>Emotion in user interface, voice interaction system</title><year>2000</year><authors><author org="Dept. of Production, Inf. &amp; Syst. Eng., Tokyo Metropolitan Inst. of Technol., Japan" id="2937537182">V. Kostov</author><author org="" id="2485838378">S. Fukuda</author></authors><n_citation>27</n_citation><doc_type>Conference</doc_type><references><reference>54301349</reference><reference>55858950</reference><reference>90671984</reference><reference>1966279822</reference><reference>1977634317</reference><reference>2082445899</reference></references><venue id="1170695740" type="C">Systems, Man and Cybernetics</venue><doi>10.1109/ICSMC.2000.885947</doi><keywords><keyword weight="0.49883">Sadness</keyword><keyword weight="0.56846">Speech synthesis</keyword><keyword weight="0.61686">Voice analysis</keyword><keyword weight="0.53853">Human voice</keyword><keyword weight="0.45247">Disgust</keyword><keyword weight="0.38966">Computer science</keyword><keyword weight="0.5129">Utterance</keyword><keyword weight="0.44728">Speech recognition</keyword><keyword weight="0.42603">Natural language processing</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.51102">Formant</keyword><keyword weight="0.50969">User interface</keyword></keywords><publisher>IEEE</publisher><abstract>An approach towards a personalized voice-emotion user interface regardless of the speakeru0027s age, sex or language is presented. An extensive set of carefully chosen utterances provided a speech database for investing acoustic similarities among eight emotional states: (unemotional) neutral, anger, sadness, happiness, disgust, surprised, stressed/troubled and scared. Based on those results, a voice interaction system (VIS) capable of sensing the useru0027s emotional message was developed. In efforts to detect emotions, several primary parameters from human speech were analyzed: pitch, formants, tempo (rhythm) and power of human voice. First the individual basic speakeru0027s voice characteristics were extracted (pitch or/and formants in neutral speech, normal speech rate, neutral speech power) and based on those parameters the emotional message of the subjectu0027s utterance was successfully extracted. The VIS interacts with the user while changing its response according to the useru0027s utterances.</abstract></paper>