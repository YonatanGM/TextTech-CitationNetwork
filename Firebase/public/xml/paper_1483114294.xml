<paper id="1483114294"><title>TIGHT WORST-CASE LOSS BOUNDS FOR PREDICTING WITH EXPERT ADVICE</title><year>1994</year><authors><author org="University of California Santa Cruz" id="2051598476">David Haussler</author><author org="University of California Santa Cruz" id="1975490744">Jyrki Kivinen</author><author org="University of California Santa Cruz" id="1977649925">Manfred K. Warmuth</author></authors><n_citation>75</n_citation><doc_type>Conference</doc_type><references><reference>1970483275</reference><reference>1979747077</reference><reference>1992392549</reference><reference>2008392312</reference><reference>2024047326</reference><reference>2050660892</reference><reference>2069317438</reference><reference>2102426343</reference><reference>2113633207</reference><reference>2148844225</reference><reference>2611627047</reference></references><venue id="2740279309" type="C">European Conference on Computational Learning Theory</venue><doi>10.1007/3-540-59119-2_169</doi><keywords><keyword weight="0.49436">Binary logarithm</keyword><keyword weight="0.43571">Discrete mathematics</keyword><keyword weight="0.57387">Upper and lower bounds</keyword><keyword weight="0.45761">Probability measure</keyword><keyword weight="0.41174">Omega</keyword><keyword weight="0.43401">Probabilistic logic</keyword><keyword weight="0.47613">Logarithm</keyword><keyword weight="0.40975">Mathematics</keyword><keyword weight="0.46037">Binary number</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>We consider on-line algorithms for predicting binary or continuous-valued outcomes, when the algorithm has available the predictions made by N experts. For a sequence of trials, we compute total losses for both the algorithm and the experts under a loss function. At the end of the trial sequence, we compare the total loss of the algorithm to the total loss of the best expert, i.e., the expert with the least loss on the particular trial sequence. We show that for a large class of loss functions, with binary outcomes the total loss of the algorithm proposed by Vovk exceeds the total loss of the best expert at most by the amount c ln N, where c is a constant determined by the loss function. This upper bound does not depend on any assumptions on how the expertsu0027u0027 predictions or the outcomes are generated, and the trial sequence can be arbitrarily long. We give a straightforward method for finding the correct value c and show by a lower bound that for this value of c, the upper bound is asymptotically tight. The lower bound is based on a probabilistic adversary argument. The class of loss functions for which the c ln N upper bound holds includes the square loss, the logarithmic loss, and the Hellinger loss. We also consider another class of loss functions, including the absolute loss, for which we have an Omega((l log N)^(1/2)) lower bound, where l is the number of trials. We show that for the square and logarithmic loss functions, Vovku0027u0027s algorithm achieves the same worst-case upper bounds with continuous-valued outcomes as with binary outcomes. For the absolute loss, we show how bounds earlier achieved for binary outcomes can be achieved with continuous-valued outcomes using a slightly more complicated algorithm.</abstract></paper>