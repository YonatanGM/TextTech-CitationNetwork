<paper id="1570871893"><title>Margin Distribution Bounds on Generalization</title><year>1999</year><authors><author org="University   of   London" id="2215287472">John Shawe-Taylor</author><author org="University   of   London" id="156417696">Nello Cristianini</author></authors><n_citation>37</n_citation><doc_type>Conference</doc_type><references><reference>1500284251</reference><reference>1574111330</reference><reference>1975846642</reference><reference>1979711143</reference><reference>1998255116</reference><reference>2040293685</reference><reference>2098774896</reference><reference>2106491486</reference><reference>2119821739</reference><reference>2156909104</reference></references><venue id="2740279309" type="C">European Conference on Computational Learning Theory</venue><doi>10.1007/3-540-49097-3_21</doi><keywords><keyword weight="0.4628">Kernel (linear algebra)</keyword><keyword weight="0.44952">Discrete mathematics</keyword><keyword weight="0.62387">Margin (machine learning)</keyword><keyword weight="0.50712">Support vector machine</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.61881">Margin classifier</keyword><keyword weight="0.47184">Statistical theory</keyword><keyword weight="0.52252">Perceptron</keyword><keyword weight="0.42874">Mathematics</keyword><keyword weight="0.417">Machine learning</keyword><keyword weight="0.47861">Sample size determination</keyword><keyword weight="0.51228">Bounded function</keyword></keywords><publisher>Springer-Verlag</publisher><abstract>A number of results have bounded generalization of a classi fier in terms of its margin on the training points. There has been some debate about whether the minimum margin is the best measure of the distribution of training set margin values with which to estimate the generalization. Freund and Schapire [6] have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on-line learning algorithm for a perceptron, as well as an expected error bound. We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size. We also derive an algorithm for optimizing the new measure for general kernel based learning machines. Some preliminary experiments are presented.</abstract></paper>