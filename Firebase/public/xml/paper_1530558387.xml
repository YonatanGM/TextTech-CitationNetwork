<paper id="1530558387"><title>Learning from Positive Data</title><year>1996</year><authors><author org="Oxford University, Computing Laboratory" id="735181462">Stephen Muggleton</author></authors><n_citation>160</n_citation><doc_type>Conference</doc_type><references><reference>69291052</reference><reference>1580451812</reference><reference>1965254321</reference><reference>1969005071</reference><reference>2002663765</reference><reference>2019363670</reference><reference>2045373010</reference><reference>2076343783</reference><reference>2090559885</reference><reference>2092386826</reference><reference>2119831128</reference><reference>2134980541</reference></references><venue id="1200006883" type="C">Inductive Logic Programming</venue><doi>10.1007/3-540-63494-0_65</doi><keywords><keyword weight="0.59499">Inductive logic programming</keyword><keyword weight="0.48231">Rule-based machine translation</keyword><keyword weight="0.48784">PROGOL</keyword><keyword weight="0.47664">Upper and lower bounds</keyword><keyword weight="0.43629">Computer science</keyword><keyword weight="0.45088">Theoretical computer science</keyword><keyword weight="0.50124">Posterior probability</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.50532">Regular language</keyword><keyword weight="0.44472">Machine learning</keyword><keyword weight="0.49546">Bayesian probability</keyword><keyword weight="0.50069">Bayes' theorem</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>Gold showed in 1967 that not even regular grammars can be exactly identified from positive examples alone. Since it is known that children learn natural grammars almost exclusively from positives examples, Goldu0027s result has been used as a theoretical support for Chomskyu0027s theory of innate human linguistic abilities. In this paper new results are presented which show that within a Bayesian framework not only grammars, but also logic programs are learnable with arbitrarily low expected error from positive examples only. In addition, we show that the upper bound for expected error of a learner which maximises the Bayesu0027 posterior probability when learning from positive examples is within a small additive term of one which does the same from a mixture of positive and negative examples. An Inductive Logic Programming implementation is described which avoids the pitfalls of greedy search by global optimisation of this function during the local construction of individual clauses of the hypothesis. Results of testing this implementation on artificially-generated data-sets are reported. These results are in agreement with the theoretical predictions.</abstract></paper>