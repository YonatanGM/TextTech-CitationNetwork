<paper id="1553262910"><title>Committee-based sampling for training probabilistic classifiers</title><year>1995</year><authors><author org="Department of Mathematics and Computer Science, Bar-Ilan University, 52900 Ramat Gan, Israel" id="356610775">Ido Dagan</author><author org="Department of Mathematics and Computer Science, Bar-Ilan University, 52900 Ramat Gan, Israel" id="2617163012">Sean P. Engelson</author></authors><n_citation>304</n_citation><doc_type>Conference</doc_type><references><reference>1513874326</reference><reference>1977182536</reference><reference>1989445634</reference><reference>2080021732</reference><reference>2084310470</reference><reference>2088538739</reference><reference>2099247782</reference><reference>2110327402</reference><reference>2115305054</reference><reference>2139709458</reference><reference>2151023586</reference><reference>2157693466</reference><reference>2162995740</reference><reference>2166856932</reference><reference>2167434254</reference></references><venue id="1180662882" type="C">International Conference on Machine Learning</venue><doi>10.1016/B978-1-55860-377-6.50027-X</doi><keywords><keyword weight="0.0">Training set</keyword><keyword weight="0.46972">Data mining</keyword><keyword weight="0.45778">Computer science</keyword><keyword weight="0.0">Information gain</keyword><keyword weight="0.49911">Probability distribution</keyword><keyword weight="0.48414">Natural language</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.55839">Sampling (statistics)</keyword><keyword weight="0.53987">Probabilistic logic</keyword><keyword weight="0.55684">Hidden Markov model</keyword><keyword weight="0.48035">Machine learning</keyword></keywords><publisher>Morgan Kaufmann</publisher><abstract>Abstract In many real-world learning tasks, it is expensive to acquire a sufficient number of labeled examples for training. This paper proposes a general method for efficiently training probabilistic classifiers, by selecting for training only the more informative examples in a stream of unlabeled examples. The method, committee-based sampling , evaluates the informativeness of an example by measuring the degree of disagreement between several model variants. These variants (the committee) are drawn randomly from a probability distribution conditioned by the training set selected so far (Monte-Carlo sampling). The method is particularly attractive because it evaluates the expected information gain from a training example implicitly, making the model both easy to implement and generally applicable. We further show how to apply committee-based sampling for training Hidden Markov Model classifiers, which are commonly used for complex classification tasks. The method was implemented and tested for the task of tagging words in natural language sentences with parts-of-speech. Experimental evaluation of committee-based sampling versus standard sequential training showed a substantial improvement in training efficiency.</abstract></paper>