<paper id="1621538476"><title>Computing Minimum and Maximum Reachability Times in Probabilistic Systems</title><year>1999</year><authors><author org="University of California at Berkeley" id="2129359981">Luca de Alfaro</author></authors><n_citation>105</n_citation><doc_type>Conference</doc_type><references><reference>43147964</reference><reference>1514560100</reference><reference>1601373402</reference><reference>1793726001</reference><reference>1850900101</reference><reference>1972203711</reference><reference>1974090207</reference><reference>2051580064</reference><reference>2081287319</reference><reference>2125500714</reference><reference>2142008356</reference></references><venue id="1145706541" type="C">International Conference on Concurrency Theory</venue><doi>10.1007/3-540-48320-9_7</doi><keywords><keyword weight="0.47914">Mathematical optimization</keyword><keyword weight="0.62582">Markov process</keyword><keyword weight="0.54716">Shortest path problem</keyword><keyword weight="0.53808">Nondeterministic algorithm</keyword><keyword weight="0.62848">Partially observable Markov decision process</keyword><keyword weight="0.44841">Computer science</keyword><keyword weight="0.63204">Markov model</keyword><keyword weight="0.60792">Markov chain</keyword><keyword weight="0.4624">Algorithm</keyword><keyword weight="0.62894">Markov decision process</keyword><keyword weight="0.55916">Probabilistic logic</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>A Markov decision process is a generalization of a Markov chain in which both probabilistic and nondeterministic choice coexist. Given a Markov decision process with costs associated with the transitions and a set of target states, the stochastic shortest path problem consists in computing the minimum expected cost of a control strategy that guarantees to reach the target. In this paper, we consider the classes of stochastic shortest path problems in which the costs are all non-negative, or all non-positive. Previously, these two classes of problems could be solved only under the assumption that the policies that minimize or maximize the expected cost also lead to the target with probability 1. This assumption does not necessarily hold for Markov decision processes that arise as model for distributed probabilistic systems. We present efficient methods for solving these two classes of problems without relying on additional assumptions. The methods are based on algorithms to transform the original problems into problems that satisfy the required assumptions. The methods lead to the efficient solution of two basic problems in the analysis of the reliability and performance of partially-specified systems: the computation of the minimum (or maximum) probability of reaching a target set, and the computation of the minimum (or maximum) expected time to reach the set.</abstract></paper>