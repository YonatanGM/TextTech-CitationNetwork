<paper id="174126042"><title>A Framework for Low Level Feature Extraction</title><year>1994</year><authors><author org="Universität Bonn &gt;  &gt;  &gt;  &gt;" id="1857454961">Wolfgang Förstner</author></authors><n_citation>237</n_citation><doc_type>Conference</doc_type><references><reference>1602607367</reference><reference>1667165204</reference><reference>1805579315</reference><reference>2002923387</reference><reference>2045870530</reference><reference>2136868517</reference><reference>2141198810</reference><reference>2145023731</reference><reference>2155487652</reference><reference>2164488798</reference></references><venue id="1124077590" type="C">European Conference on Computer Vision</venue><doi>10.1007/BFb0028370</doi><keywords><keyword weight="0.47514">Computer vision</keyword><keyword weight="0.0">Feature detection</keyword><keyword weight="0.48647">Pattern recognition</keyword><keyword weight="0.65805">Edge detection</keyword><keyword weight="0.44044">Computer science</keyword><keyword weight="0.61988">Scale space</keyword><keyword weight="0.45488">Exploit</keyword><keyword weight="0.70187">Feature extraction</keyword><keyword weight="0.0">Local statistics</keyword><keyword weight="0.0">Artificial intelligence</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>The paper presents a framework for extracting low level features. Its main goal is to explicitely exploit the information content of the image as far as possible. This leads to new techniques for deriving image parameters, to either the elimination or the elucidation of ”buttons”, like thresholds, and to interpretable quality measures for the results, which may be used in subsequent steps. Feature extraction is based on local statistics of the image function. Methods are available for blind estimation of a signal dependent noise variance, for feature preserving restoration, for feature detection and classification, and for the location of general edges and points. Their favorable scale space properties are discussed.</abstract></paper>