<paper id="1583555936"><title>Bayes point machines</title><year>2001</year><authors><author org="Microsoft Research, St George House, 1 Guildhall Street, CB2 3NH Cambridge, United Kingdom#TAB#" id="1294330762">Ralf Herbrich</author><author org="Technical University of Berlin, Franklinstr. 28/29, 10587 Berlin, Germany#TAB#" id="2032008572">Thore Graepel</author><author org="Department of Engineering Mathematics, Bristol University, BS8 1TR Bristol, United Kingdom#TAB#" id="2130015035">Colin Campbell</author></authors><n_citation>183</n_citation><doc_type>Journal</doc_type><references><reference>206794860</reference><reference>854322902</reference><reference>1505375788</reference><reference>1538429986</reference><reference>1539928323</reference><reference>1540007258</reference><reference>1540155273</reference><reference>2009207944</reference><reference>2014384147</reference><reference>2029029543</reference><reference>2087347434</reference><reference>2088032561</reference><reference>2106491486</reference><reference>2115438515</reference><reference>2117063635</reference><reference>2126291774</reference><reference>2141274633</reference><reference>2149684865</reference><reference>2156622608</reference><reference>2156909104</reference><reference>2158388185</reference><reference>2159737176</reference><reference>2167277498</reference><reference>2293789987</reference></references><venue id="118988714" type="J">Journal of Machine Learning Research</venue><doi>10.1162/153244301753683717</doi><keywords><keyword weight="0.59496">Structured support vector machine</keyword><keyword weight="0.62856">Naive Bayes classifier</keyword><keyword weight="0.45709">Pattern recognition</keyword><keyword weight="0.60558">Supervised learning</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.59627">Relevance vector machine</keyword><keyword weight="0.64089">Margin classifier</keyword><keyword weight="0.60769">Kernel method</keyword><keyword weight="0.6398">Bayes error rate</keyword><keyword weight="0.59844">Perceptron</keyword><keyword weight="0.45365">Machine learning</keyword><keyword weight="0.4166">Mathematics</keyword></keywords><publisher>JMLR.org</publisher><abstract>Kernel-classifiers comprise a powerful class of non-linear decision functions for binary classification. The support vector machine is an example of a learning algorithm for kernel classifiers that singles out the consistent classifier with the largest margin, i.e. minimal real-valued output on the training sample, within the set of consistent hypotheses, the so-called version space. We suggest the Bayes point machine as a well-founded improvement which approximates the Bayes-optimal decision by the centre of mass of version space. We present two algorithms to stochastically approximate the centre of mass of version space: a billiard sampling algorithm and a sampling algorithm based on the well known perceptron algorithm. It is shown how both algorithms can be extended to allow for soft-boundaries in order to admit training errors. Experimentally, we find that - for the zero training error case - Bayes point machines consistently outperform support vector machines on both surrogate data and real-world benchmark data sets. In the soft-boundary/soft-margin case, the improvement over support vector machines is shown to be reduced. Finally, we demonstrate that the real-valued output of single Bayes points on novel test points is a valid confidence measure and leads to a steady decrease in generalisation error when used as a rejection criterion.</abstract></paper>