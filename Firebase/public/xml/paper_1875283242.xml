<paper id="1875283242"><title>Classification Using Bayes Averaging of Multiple, Relational Rule-based Models</title><year>1996</year><authors><author org="University of California" id="2465769383">Kamal M. Ali</author><author org="University of California" id="1996789426">Michael J. Pazzani</author></authors><n_citation>3</n_citation><doc_type>Conference</doc_type><references><reference>83160502</reference><reference>135311109</reference><reference>145862677</reference><reference>176843356</reference><reference>1482451543</reference><reference>1516545621</reference><reference>1565236324</reference><reference>1580451812</reference><reference>1999138184</reference><reference>2037689320</reference><reference>2157799908</reference><reference>2563693766</reference></references><venue id="2622962978" type="C">International Conference on Artificial Intelligence and Statistics</venue><doi>10.1007/978-1-4612-2404-4_20</doi><keywords><keyword weight="0.527">Default rule</keyword><keyword weight="0.48739">Rule-based system</keyword><keyword weight="0.56725">Relational database</keyword><keyword weight="0.4391">Computer science</keyword><keyword weight="0.48853">Uniform distribution (continuous)</keyword><keyword weight="0.6134">Posterior probability</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.54307">Prior probability</keyword><keyword weight="0.48483">Recursion</keyword><keyword weight="0.45431">Machine learning</keyword><keyword weight="0.51646">Bayes' theorem</keyword></keywords><publisher>Springer, New York, NY</publisher><abstract>We present a way of approximating the posterior probability of a rule-set model that is comprised of a set of class descriptions. Each class description, in turn, consists of a set of relational rules. The ability to compute this posterior and to learn many models from the same training set allows us to approximate the expectation that an example to be classified belongs to some class. The example is assigned to the class maximizing the expectation. By assuming a uniform prior distribution of models, the posterior of the model does not depend on the structure of the model: it only depends on how the training examples are partitioned by the rules of the rule-set model. This uniform distribution assumption allows us to compute the posterior for models containing relational and recursive rules. Our approximation to the posterior probability yields significant improvements in accuracy as measured on four relational data sets and four attribute-value data sets from the UCI repository. We also provide evidence that learning multiple models helps most in data sets in which there are many, apparently equally good rules to learn.</abstract></paper>