<paper id="1976026757"><title>On Weak Learning</title><year>1995</year><authors><author org="Univ Calif Santa Cruz, Board Studies Comp &amp; Informat Sci, Santa Cruz, CA 95064, USA and Fujitsu Labs Ltd, Inst Sis, Iias, Numazu, Japan#TAB#" id="2024443744">David P. Helmbold</author><author org="Univ Calif Santa Cruz, Board Studies Comp &amp; Informat Sci, Santa Cruz, CA 95064, USA and Fujitsu Labs Ltd, Inst Sis, Iias, Numazu, Japan#TAB#" id="1977649925">Manfred K. Warmuth</author></authors><n_citation>69</n_citation><doc_type>Conference</doc_type><references><reference>1655990431</reference><reference>1969427413</reference><reference>1971361630</reference><reference>2019363670</reference><reference>2029599246</reference><reference>2046995465</reference><reference>2050660892</reference><reference>2057566401</reference><reference>2066688546</reference><reference>2075121617</reference><reference>2079866219</reference><reference>2087907345</reference><reference>2142399242</reference><reference>2142471335</reference><reference>2154952480</reference><reference>2611627047</reference></references><venue id="1177622950" type="C">Conference on Learning Theory</venue><doi>10.1006/jcss.1995.1044</doi><keywords><keyword weight="0.54688">Polynomial</keyword><keyword weight="0.4376">Computer science</keyword><keyword weight="0.52766">Oracle</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.55278">Time complexity</keyword><keyword weight="0.60837">Population-based incremental learning</keyword><keyword weight="0.62353">Weighted Majority Algorithm</keyword><keyword weight="0.52193">Complexity class</keyword><keyword weight="0.45021">Combinatorics</keyword><keyword weight="0.60603">Concept class</keyword><keyword weight="0.52648">Gibbs algorithm</keyword><keyword weight="0.46496">Algorithm</keyword><keyword weight="0.44553">Machine learning</keyword></keywords><publisher>Academic Press</publisher><abstract>An algorithm is a weak learning algorithm if with some small probability it outputs a hypothesis with error slightly below 50%. This paper presents relationships between weak learning, weak prediction (where the probability of being correct is slightly larger than 50%), and consistency oracles (which decide whether or not a given set of examples is consistent with a concept in the class). Our main result is a simple polynomial prediction algorithm which makes only a single query to a consistency oracle and whose predictions have a polynomial edge over random guessing. We compare this prediction algorithm with several of the standard prediction techniques, deriving an improved worst case bound on Gibbs algorithm in the process. We use our algorithm to show that a concept class is polynomially learnable if and only if there is a polynomial probabilistic consistency oracle for the class. Since strong learning algorithms can be built from weak learning algorithms, our results also characterizes strong learnability.</abstract></paper>