<paper id="1964357740"><title>A tutorial on support vector regression</title><year>2004</year><authors><author org="RSISE, Australian National University, Canberra 0200, Australia. Alex.Smola@anu.edu.au#TAB#" id="1972291593">Alex J. Smola</author><author org="Max-Planck-Institut für biologische Kybernetik, 72076 Tübingen, Germany. Bernhard.Schoelkopf@tuebingen.mpg.de#TAB#" id="297432538">Bernhard Schölkopf</author></authors><n_citation>5996</n_citation><doc_type>Journal</doc_type><references><reference>26816478</reference><reference>109020964</reference><reference>854322902</reference><reference>1486540228</reference><reference>1517084761</reference><reference>1540155273</reference><reference>1548139318</reference><reference>1551209770</reference><reference>1604585277</reference><reference>1605479404</reference><reference>1638203394</reference><reference>1663792126</reference><reference>1749992802</reference><reference>1936866406</reference><reference>1973756582</reference><reference>1981315194</reference><reference>1986931325</reference><reference>1995558350</reference><reference>1998438337</reference><reference>2007154098</reference><reference>2017753243</reference><reference>2036350498</reference><reference>2041099987</reference><reference>2041404167</reference><reference>2047542122</reference><reference>2054658115</reference><reference>2055522016</reference><reference>2064575768</reference><reference>2077896690</reference><reference>2086867325</reference><reference>2087347434</reference><reference>2088032561</reference><reference>2096974332</reference><reference>2102909657</reference><reference>2106491486</reference><reference>2108136473</reference><reference>2110899801</reference><reference>2113584252</reference><reference>2119573243</reference><reference>2119821739</reference><reference>2121649666</reference><reference>2123737232</reference><reference>2132870739</reference><reference>2137226992</reference><reference>2137512539</reference><reference>2139212933</reference><reference>2139999468</reference><reference>2140095548</reference><reference>2142334564</reference><reference>2148245374</reference><reference>2151040995</reference><reference>2153635508</reference><reference>2156512439</reference><reference>2156909104</reference><reference>2160619799</reference><reference>2161920802</reference><reference>2162657883</reference><reference>2164096571</reference><reference>2408196097</reference></references><venue id="5437875" type="J">Statistics and Computing</venue><doi>10.1023/B:STCO.0000035301.49549.88</doi><keywords><keyword weight="0.51331">Structured support vector machine</keyword><keyword weight="0.53586">Least squares support vector machine</keyword><keyword weight="0.44408">Computer science</keyword><keyword weight="0.50961">Support vector machine</keyword><keyword weight="0.44802">Quadratic equation</keyword><keyword weight="0.41373">Regular polygon</keyword><keyword weight="0.44503">Regularization (mathematics)</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.52858">Relevance vector machine</keyword><keyword weight="0.45015">Machine learning</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.</abstract></paper>