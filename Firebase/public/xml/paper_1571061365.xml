<paper id="1571061365"><title>Prototype and feature selection by sampling and random mutation hill climbing algorithms</title><year>1994</year><authors><author org="Department of Computer Science, Univ. of Massachusetts, Amherst, MA" id="56345256">David B. Skalak</author></authors><n_citation>397</n_citation><doc_type>Conference</doc_type><references><reference>23418094</reference><reference>101142848</reference><reference>190437827</reference><reference>205184011</reference><reference>1483055888</reference><reference>1527462214</reference><reference>1533544838</reference><reference>1539739406</reference><reference>1553244859</reference><reference>1579055868</reference><reference>1592894394</reference><reference>1619226191</reference><reference>1994410331</reference><reference>1995023359</reference><reference>2023696990</reference><reference>2053913299</reference><reference>2057211203</reference><reference>2063270381</reference><reference>2098631313</reference><reference>2102009083</reference><reference>2107686700</reference><reference>2125055259</reference><reference>2132166479</reference></references><venue id="1180662882" type="C">International Conference on Machine Learning</venue><doi>10.1016/B978-1-55860-335-6.50043-X</doi><keywords><keyword weight="0.60776">Hill climbing</keyword><keyword weight="0.52706">Feature selection</keyword><keyword weight="0.59935">Best bin first</keyword><keyword weight="0.43617">Computer science</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.52096">Nearest-neighbor chain algorithm</keyword><keyword weight="0.51945">Cluster analysis</keyword><keyword weight="0.58915">Nearest neighbor search</keyword><keyword weight="0.53731">k-nearest neighbors algorithm</keyword><keyword weight="0.45983">Pattern recognition</keyword><keyword weight="0.46301">Algorithm</keyword><keyword weight="0.51702">Sampling (statistics)</keyword><keyword weight="0.45642">Machine learning</keyword></keywords><publisher>Morgan Kaufmann</publisher><abstract>With the goal of reducing computational costs without sacrificing accuracy, we describe two algorithms to find sets of prototypes for nearest neighbor classification. Here, the term “prototypes” refers to the reference instances used in a nearest neighbor computation — the instances with respect to which similarity is assessed in order to assign a class to a new data item. Both algorithms rely on stochastic techniques to search the space of sets of prototypes and are simple to implement. The first is a Monte Carlo sampling algorithm; the second applies random mutation hill climbing. On four datasets we show that only three or four prototypes sufficed to give predictive accuracy equal or superior to a basic nearest neighbor algorithm whose run-time storage costs were approximately 10 to 200 times greater. We briefly investigate how random mutation hill climbing may be applied to select features and prototypes simultaneously. Finally, we explain the performance of the sampling algorithm on these datasets in terms of a statistical measure of the extent of clustering displayed by the target classes.</abstract></paper>