<paper id="1494780677"><title>Boosting and other machine learning algorithms</title><year>1994</year><authors><author org="AT&amp;T Bell Labs.,Holmdel, NJ#TAB#" id="2776087671">Harris Drucker</author><author org="" id="2134830209">Corinna Cortes</author><author org="" id="2071363206">Lawrence D. Jackel</author><author org="" id="2053214915">Yann LeCun</author><author org="" id="2022407533">Vladimir Vapnik</author></authors><n_citation>30</n_citation><doc_type>Conference</doc_type><references><reference>2056763477</reference><reference>2087347434</reference><reference>2109779438</reference><reference>2117799453</reference><reference>2119113516</reference><reference>2135293965</reference><reference>2137291015</reference><reference>2154231986</reference><reference>2154579312</reference><reference>2166501286</reference><reference>2168228682</reference></references><venue id="1180662882" type="C">International Conference on Machine Learning</venue><doi>10.1016/B978-1-55860-335-6.50015-5</doi><keywords><keyword weight="0.43994">Computer science</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.58977">Computational learning theory</keyword><keyword weight="0.54612">Artificial neural network</keyword><keyword weight="0.56402">Ensemble learning</keyword><keyword weight="0.69038">BrownBoost</keyword><keyword weight="0.46247">Pattern recognition</keyword><keyword weight="0.454">Algorithm</keyword><keyword weight="0.4726">Optical character recognition</keyword><keyword weight="0.66098">Boosting (machine learning)</keyword><keyword weight="0.63449">LPBoost</keyword><keyword weight="0.46706">Machine learning</keyword><keyword weight="0.66414">Gradient boosting</keyword></keywords><publisher>Morgan Kaufmann</publisher><abstract>In an optical character recognition problem, we compare (as a function of training set size) the performance of three neural network based ensemble methods (two versions of boosting and a committee of neural networks trained independently) to that of a single network. In boosting, the number of patterns actually used for training is a subset of all potential training patterns. Based on either a fixed computational cost or training set size criterion, some version of boosting is best We also compare (for a fixed training set size) boosting to the following algorithms: optimal margin classifiers, tangent distance, local learning, k-nearest neighbor, and a large weight sharing network with the boosting algorithm showing the best performance.</abstract></paper>