<paper id="1840338487"><title>On Bias, Variance, 0/1—Loss, and the Curse-of-Dimensionality</title><year>1997</year><authors><author org="Department of Statistics and Stanford Linear Accelerator Center, Stanford University" id="2157091523">Jerome H. Friedman</author></authors><n_citation>745</n_citation><doc_type>Journal</doc_type><references><reference>1516193414</reference><reference>1517993545</reference><reference>1625504505</reference><reference>2067374717</reference><reference>2076118331</reference><reference>2125055259</reference><reference>2132166479</reference><reference>2912934387</reference></references><venue id="121920818" type="J">Data Mining and Knowledge Discovery</venue><doi>10.1023/A:1009778005914</doi><keywords><keyword weight="0.60022">Classification rule</keyword><keyword weight="0.59976">Naive Bayes classifier</keyword><keyword weight="0.44392">Pattern recognition</keyword><keyword weight="0.41111">Computer science</keyword><keyword weight="0.57134">Mean squared error</keyword><keyword weight="0.52787">Curse of dimensionality</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.44623">Statistics</keyword><keyword weight="0.68475">Bayes error rate</keyword><keyword weight="0.43941">Machine learning</keyword><keyword weight="0.54169">Bayes' theorem</keyword><keyword weight="0.55328">Estimator</keyword></keywords><publisher>Kluwer Academic Publishers</publisher><abstract>The classification problem is considered in which an output variable y assumes discrete values with respective probabilities that depend upon the simultaneous values of a set of input variables x = {x_1,....,x_n}. At issue is how error in the estimates of these probabilities affects classification error when the estimates are used in a classification rule. These effects are seen to be somewhat counter intuitive in both their strength and nature. In particular the bias and variance components of the estimation error combine to influence classification in a very different way than with squared error on the probabilities themselves. Certain types of (very high) bias can be canceled by low variance to produce accurate classification. This can dramatically mitigate the effect of the bias associated with some simple estimators like “naive” Bayes, and the bias induced by the curse-of-dimensionality on nearest-neighbor procedures. This helps explain why such simple methods are often competitive with and sometimes superior to more sophisticated ones for classification, and why “bagging/aggregating” classifiers can often improve accuracy. These results also suggest simple modifications to these procedures that can (sometimes dramatically) further improve their classification performance.</abstract></paper>