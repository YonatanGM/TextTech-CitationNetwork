<paper id="1039852248"><title>Confidently Assessing a Zero Probability of Software Failure</title><year>1993</year><authors><author org="" id="568848126">Jeffrey M. Voas</author><author org="Reliable Software Technologies Corporation" id="2702064458">Christoph C. Michael</author><author org="College of William &amp; Mary" id="2792816725">Keith W. Miller</author></authors><n_citation>18</n_citation><doc_type>Conference</doc_type><references><reference>1905291772</reference><reference>1969042161</reference><reference>2049695835</reference><reference>2094197777</reference><reference>2099855131</reference><reference>2104911885</reference><reference>2113004249</reference><reference>2122215370</reference><reference>2142812175</reference><reference>2164501290</reference></references><venue id="1128163780" type="C">International Conference on Computer Safety, Reliability, and Security</venue><doi>10.1007/978-1-4471-2061-2_21</doi><keywords><keyword weight="0.0">Software fault</keyword><keyword weight="0.56706">Random testing</keyword><keyword weight="0.44818">Computer science</keyword><keyword weight="0.0">Probability of failure</keyword><keyword weight="0.0">Software failure</keyword><keyword weight="0.62581">Software</keyword><keyword weight="0.68912">Software quality</keyword><keyword weight="0.47262">Reliability engineering</keyword></keywords><publisher>Springer, London</publisher><abstract>Randomly generated software tests are an established method of estimating software reliability [5, 7]. But as software applications require higher and higher reliabilities, practical difficulties with random testing have become increasingly problematic. These practical problems are particularly acute in life-critical applications, where requirements of 10−7failures per hour of system reliability translate into a probability of failure (pof) of perhaps 10−9 or less for each individual execution of the software [4]. We refer to software with reliability requirements of this magnitude as ultra-reliable software.</abstract></paper>