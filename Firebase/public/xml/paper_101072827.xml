<paper id="101072827"><title>A comparative study on the use of labeled and unlabeled data for large margin classifiers</title><year>2004</year><authors><author org="Precision and Intelligence Laboratory; Tokyo Institute of Technology; Yokohama Japan" id="2170796186">Hiroya Takamura</author><author org="Precision and Intelligence Laboratory; Tokyo Institute of Technology; Yokohama Japan" id="2165400638">Manabu Okumura</author></authors><n_citation>3</n_citation><doc_type>Conference</doc_type><references><reference>1512558044</reference><reference>1988995507</reference><reference>2007463795</reference><reference>2097089247</reference><reference>2106868411</reference><reference>2107008379</reference><reference>2149684865</reference><reference>2158388185</reference><reference>2166023018</reference><reference>2166473218</reference></references><venue id="1126706392" type="C">International Joint Conference on Natural Language Processing</venue><doi>10.1007/978-3-540-30211-7_48</doi><keywords><keyword weight="0.66332">Semi-supervised learning</keyword><keyword weight="0.45232">Pattern recognition</keyword><keyword weight="0.61637">Naive Bayes classifier</keyword><keyword weight="0.4033">Computer science</keyword><keyword weight="0.47301">Latent variable</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.41724">Generative grammar</keyword><keyword weight="0.0">Labeled data</keyword><keyword weight="0.0">Text categorization</keyword><keyword weight="0.63456">Fisher kernel</keyword><keyword weight="0.44841">Machine learning</keyword><keyword weight="0.62127">Generative model</keyword></keywords><publisher>Springer, Berlin, Heidelberg</publisher><abstract>We propose to use both labeled and unlabeled data with the Expectation-Maximization (EM) algorithm in order to estimate the generative model and use this model to construct a Fisher kernel. The Naive Bayes generative probability is used to model a document. Through the experiments of text categorization, we empirically show that, (a) the Fisher kernel with labeled and unlabeled data outperforms Naive Bayes classifiers with EM and other methods for a sufficient amount of labeled data, (b) the value of additional unlabeled data diminishes when the labeled data size is large enough for estimating a reliable model, (c) the use of categories as latent variables is effective, and (d) larger unlabeled training datasets yield better results.</abstract></paper>