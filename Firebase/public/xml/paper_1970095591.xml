<paper id="1970095591"><title>Evaluation of information retrieval systems: A decision theory approach</title><year>1978</year><authors><author org="School of Librarianship, University of California, Berkeley, Berkeley, CA 94720" id="2143014860">Donald H. Kraft</author><author org="Graduate Library School, University of Chicago, Chicago, IL 60637" id="1999734350">Abraham Bookstein</author></authors><n_citation>47</n_citation><doc_type>Journal</doc_type><references><reference>1978248973</reference><reference>1982442952</reference><reference>2017270976</reference><reference>2041565863</reference><reference>2056152306</reference><reference>2057887972</reference><reference>2058413358</reference><reference>2059782624</reference><reference>2080068076</reference><reference>2082729696</reference><reference>2087635845</reference><reference>2119529697</reference><reference>2153303737</reference></references><venue id="80113298" type="J">Journal of the Association for Information Science and Technology</venue><doi>10.1002/asi.4630290106</doi><keywords><keyword weight="0.56334">Divergence-from-randomness model</keyword><keyword weight="0.45384">Data mining</keyword><keyword weight="0.59686">Optimal decision</keyword><keyword weight="0.44303">Computer science</keyword><keyword weight="0.0">Artificial intelligence</keyword><keyword weight="0.57864">Decision theory</keyword><keyword weight="0.56697">Vector space model</keyword><keyword weight="0.56701">Term Discrimination</keyword><keyword weight="0.63127">Decision rule</keyword><keyword weight="0.45254">Information retrieval</keyword><keyword weight="0.60492">Precision and recall</keyword><keyword weight="0.56174">Relevance (information retrieval)</keyword><keyword weight="0.45435">Machine learning</keyword></keywords><publisher>John Wiley &amp; Sons, Ltd</publisher><abstract>The Swets model of information retrieval, based on a decision theory approach, is discussed, with the overall performance measure being the crucial element reexamined in this paper. The Neyman-Pearson criterion from statistical decision theory, and based on likelihood ratios, is used to determine an optimal range of Z, the variable assigned to each document by the retrieval system in an attempt to discriminate between relevant and nonrelevant documents. This criterion is shown to be directly related to both precision and recall, and is equivalent to the maximization of the expected value of the retrieval decision for a specific query and a given document under certain conditions. Thus, a compromise can be reached between those who advocate precision as a measure, due partially to its ability to be easily measurable empirically, and those who advocate consideration of recall. Several cases of the normal and Poisson distributions for the variable Z are discussed in terms of their implications for the Neyman-Pearson decision rule. It is seen that when the variances are unequal, the Swets rule of retrieving a document if its Z value is large enough is not optimal. Finally, the situation of precision and recall not being inversely related is shown to be possible under certain conditions. Thus, this paper attempts to extend the understanding of the theoretical foundations of the decision theory approach to information retrieval.</abstract></paper>